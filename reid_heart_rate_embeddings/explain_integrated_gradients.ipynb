{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-madonna",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.autograph.set_verbosity(0)\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, Conv1D, GlobalMaxPooling1D, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from alibi.explainers import IntegratedGradients\n",
    "import matplotlib.pyplot as plt\n",
    "print('TF version: ', tf.__version__)\n",
    "print('Eager execution enabled: ', tf.executing_eagerly()) # True\n",
    "\n",
    "max_features = 10000\n",
    "maxlen = 100\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "test_labels = y_test.copy()\n",
    "train_labels = y_train.copy()\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "y_train, y_test = to_categorical(y_train), to_categorical(y_test)\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "index = imdb.get_word_index()\n",
    "reverse_index = {value: key for (key, value) in index.items()}\n",
    "\n",
    "def decode_sentence(x, reverse_index):\n",
    "    # the `-3` offset is due to the special tokens used by keras\n",
    "    # see https://stackoverflow.com/questions/42821330/restore-original-text-from-keras-s-imdb-dataset\n",
    "    return \" \".join([reverse_index.get(i - 3, 'UNK') for i in x])\n",
    "\n",
    "print(decode_sentence(x_test[2], reverse_index),y_test[3])\n",
    "\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "\n",
    "load_model = False\n",
    "save_model = True\n",
    "\n",
    "filepath = '../model_files/' # change to directory where model is downloaded\n",
    "model_name = 'integrated_gradients.h5'\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# strategy = tf.distribute.MirroredStrategy()\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[1], 'GPU')\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "    except RuntimeError as e:\n",
    "        # Visible devices must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "\n",
    "# with tf.device('/device:GPU:2'):\n",
    "if load_model:\n",
    "    model = tf.keras.models.load_model(os.path.join(filepath,model_name))\n",
    "else:\n",
    "    print('Build model...')\n",
    "\n",
    "    inputs = Input(shape=(maxlen,), dtype='int32')\n",
    "    embedded_sequences = Embedding(max_features,\n",
    "                                   embedding_dims)(inputs)\n",
    "    out = Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1)(embedded_sequences)\n",
    "    out = Dropout(0.4)(out)\n",
    "    out = GlobalMaxPooling1D()(out)\n",
    "    out = Dense(hidden_dims,\n",
    "                activation='relu')(out)\n",
    "    out = Dropout(0.4)(out)\n",
    "    outputs = Dense(2, activation='softmax')(out)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    print('Train...')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=256,\n",
    "              epochs=100,\n",
    "              validation_data=(x_test, y_test),\n",
    "             verbose=0)\n",
    "    if save_model:\n",
    "        if not os.path.exists(filepath):\n",
    "            os.makedirs(filepath)\n",
    "        model.save(os.path.join(filepath,model_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-branch",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 50\n",
    "method = \"gausslegendre\"\n",
    "internal_batch_size = 100\n",
    "nb_samples = 10\n",
    "ig  = IntegratedGradients(model,\n",
    "                          layer=model.layers[1],\n",
    "                          n_steps=n_steps,\n",
    "                          method=method,\n",
    "                          internal_batch_size=internal_batch_size)\n",
    "\n",
    "x_test_sample = x_test[:nb_samples]\n",
    "predictions = model(x_test_sample).numpy().argmax(axis=1)\n",
    "explanation = ig.explain(x_test_sample,\n",
    "                         baselines=None,\n",
    "                         target=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "natural-announcement",
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs = explanation.attributions\n",
    "print('Attributions shape:', attrs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contained-implement",
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs = attrs.sum(axis=2)\n",
    "print('Attributions shape:', attrs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-behalf",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 3\n",
    "x_i = x_test_sample[i]\n",
    "attrs_i = attrs[i]\n",
    "pred = predictions[i]\n",
    "pred_dict = {1: 'Positive review', 0: 'Negative review'}\n",
    "\n",
    "print('Predicted label =  {}: {}'.format(pred, pred_dict[pred]))\n",
    "\n",
    "from IPython.display import HTML\n",
    "def  hlstr(string, color='white'):\n",
    "    \"\"\"\n",
    "    Return HTML markup highlighting text with the desired color.\n",
    "    \"\"\"\n",
    "    return f\"<mark style=background-color:{color}>{string} </mark>\"\n",
    "\n",
    "def colorize(attrs, cmap='PiYG'):\n",
    "    \"\"\"\n",
    "    Compute hex colors based on the attributions for a single instance.\n",
    "    Uses a diverging colorscale by default and normalizes and scales\n",
    "    the colormap so that colors are consistent with the attributions.\n",
    "    \"\"\"\n",
    "    import matplotlib as mpl\n",
    "    cmap_bound = np.abs(attrs).max()\n",
    "    norm = mpl.colors.Normalize(vmin=-cmap_bound, vmax=cmap_bound)\n",
    "    cmap = mpl.cm.get_cmap(cmap)\n",
    "\n",
    "    # now compute hex values of colors\n",
    "    colors = list(map(lambda x: mpl.colors.rgb2hex(cmap(norm(x))), attrs))\n",
    "    return colors\n",
    "\n",
    "words = decode_sentence(x_i, reverse_index).split()\n",
    "colors = colorize(attrs_i)\n",
    "\n",
    "HTML(\"\".join(list(map(hlstr, words, colors))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "molecular-bones",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
