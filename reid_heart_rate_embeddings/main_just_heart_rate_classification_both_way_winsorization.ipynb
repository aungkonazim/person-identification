{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "X_hr2,y_stress2,y_participant2,X_time2 = pickle.load(open('../data/tabular_data_60_seconds_ppg_rr_dalia_normalized.p','rb'))\n",
    "\n",
    "X_hr2 = X_hr2.reshape(-1,30,1)\n",
    "\n",
    "import pickle\n",
    "X_hr1,y_stress1,y_participant1,X_time1 = pickle.load(open('../data/tabular_data_60_seconds_ppg_rr_normalized.p','rb'))\n",
    "\n",
    "X_hr1 = X_hr1.reshape(-1,30,1)\n",
    "\n",
    "import numpy as np\n",
    "X_hr = np.concatenate([X_hr2,X_hr1])\n",
    "\n",
    "y_participant = np.array(list(y_participant2)+list(y_participant1+np.max(y_participant1)+1))\n",
    "\n",
    "y_time = np.array(list(y_stress2)+list(y_stress1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hr,y_time,y_participant,X_acl = pickle.load(open('../data/tabular_data_60_seconds_ppg_rr_dalia_normalized.p','rb'))\n",
    "X_hr = X_hr.reshape(-1,30,1)\n",
    "X_acl = X_acl.reshape(-1,30,1)\n",
    "# X_hr = np.concatenate([X_hr,X_acl],axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.DataFrame({'x':np.arange(X_hr.shape[0]),'y':y_participant,'time':y_time})\n",
    "\n",
    "train_percentage = .6\n",
    "val_percentage = train_percentage+.1\n",
    "def split_data(df):\n",
    "    df = df.sort_values('time').reset_index(drop=True)\n",
    "    n = df.shape[0]\n",
    "    train_index = df['x'].values[:int(n*train_percentage)]\n",
    "    val_index = df['x'].values[int(n*train_percentage):int(n*val_percentage)]\n",
    "    test_index = df['x'].values[int(n*val_percentage):]\n",
    "    return pd.DataFrame({'train':[list(train_index)],'val':[list(val_index)],'test':[list(test_index)]})\n",
    "\n",
    "index_df = all_data.groupby('y',as_index=False).apply(split_data)\n",
    "\n",
    "from functools import reduce\n",
    "train_index = np.array(reduce(lambda a,b:a+b,index_df['train'].values))\n",
    "val_index = np.array(reduce(lambda a,b:a+b,index_df['val'].values))\n",
    "test_index = np.array(reduce(lambda a,b:a+b,index_df['test'].values))\n",
    "\n",
    "\n",
    "train_x,train_y = X_hr[train_index],y_participant[train_index]\n",
    "val_x,val_y = X_hr[val_index],y_participant[val_index]\n",
    "test_x,test_y = X_hr[test_index],y_participant[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Physical GPUs, 1 Logical GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape (Reshape)            (None, 30, 1)             0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 30, 100)           4100      \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 30, 50)            200050    \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 15, 50)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 15, 100)           100100    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 7, 100)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 7, 100)            400       \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 7, 100)            200100    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 3, 100)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 3, 100)            400       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 3, 100)            0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 15)                4515      \n",
      "=================================================================\n",
      "Total params: 509,665\n",
      "Trainable params: 509,265\n",
      "Non-trainable params: 400\n",
      "_________________________________________________________________\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 30, 1)]           0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 30, 1)             0         \n",
      "_________________________________________________________________\n",
      "sequential (Sequential)      (None, 15)                509665    \n",
      "_________________________________________________________________\n",
      "feature (Lambda)             (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 15)                240       \n",
      "=================================================================\n",
      "Total params: 509,905\n",
      "Trainable params: 509,505\n",
      "Non-trainable params: 400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "    except RuntimeError as e:\n",
    "        # Visible devices must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split,LeaveOneGroupOut,LeavePGroupsOut\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K \n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "%matplotlib notebook\n",
    "from keras.layers import Conv1D,Reshape,BatchNormalization,TimeDistributed, \\\n",
    "Dropout,Input,MaxPooling1D,Flatten,Dense,Input, GaussianNoise,LSTM, Bidirectional, Input\n",
    "from keras.models import Model, Sequential\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelBinarizer,LabelEncoder\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from scipy.stats.mstats import winsorize\n",
    "def get_model(input_shape=(256,4),act='relu',loss=\"categorical_crossentropy\",opt='adam',\n",
    "              n_classes=350,n_output = 15):\n",
    "    \n",
    "    \n",
    "    model =  Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "    model.add(Reshape(input_shape))\n",
    "    model.add(Conv1D(100,40,input_shape=input_shape,activation='selu',kernel_initializer='normal',padding='same'))\n",
    "#     model.add(Bidirectional(GRU(10,activation='relu',return_sequences=True)))\n",
    "    model.add(Conv1D(50,40,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "#     model.add(Bidirectional(GRU(10,activation='relu',return_sequences=True)))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Dropout(.2))\n",
    "    model.add(Conv1D(100,20,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "#     model.add(Bidirectional(GRU(10,activation='relu',return_sequences=True)))\n",
    "    model.add(BatchNormalization())\n",
    "#     model.add(Dropout(.2))\n",
    "    model.add(Conv1D(100,20,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "#     model.add(Bidirectional(GRU(10,activation='relu',return_sequences=True)))\n",
    "    model.add(BatchNormalization())\n",
    "#     model.add(Conv1D(100,20,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "#     model.add(MaxPooling1D(2))\n",
    "# #     model.add(Bidirectional(GRU(10,activation='relu',return_sequences=True)))\n",
    "#     model.add(BatchNormalization())\n",
    "    \n",
    "#     model.add(Conv1D(100,20,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "#     model.add(MaxPooling1D(2))\n",
    "#     model.add(BatchNormalization())\n",
    "    \n",
    "#     model.add(Conv1D(200,2,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Bidirectional(GRU(10,activation='relu',return_sequences=True)))\n",
    "    model.add(Dropout(.2))\n",
    "    model.add(Flatten())\n",
    "#     model.add(Dense(n_classes,activation='relu'))\n",
    "#     model.add(Dense(n_output*2,activation='relu'))\n",
    "    model.add(Dense(n_output,activation=None))\n",
    "    model.summary()\n",
    "    input_ = Input(shape=input_shape)\n",
    "    reshaped_input = Reshape(input_shape)(input_)\n",
    "    embedding = model(reshaped_input)\n",
    "    embedding = tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1),name='feature')(embedding)\n",
    "    \n",
    "#     class_embedding = Dense(n_output,activation='sigmoid')(final_embedding)\n",
    "    y_output = Dense(n_output,activation='softmax',name='output')(embedding) \n",
    "    model1 = Model(input_,y_output)\n",
    "#     model1.compile(loss={'output':tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "#                         'feature':tfa.losses.TripletSemiHardLoss()},optimizer=opt,\n",
    "#                    loss_weights={'output':1,'feature':0})\n",
    "    model1.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),optimizer='adam')\n",
    "    return model1\n",
    "\n",
    "# model1 = get_model(input_shape=(n_timesteps,1))\n",
    "# model1.summary()\n",
    "n_output = len(np.unique(train_y))\n",
    "model = get_model(input_shape=(30,1),n_classes=n_output,n_output=n_output) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_2 (Reshape)          (None, 30, 1)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 30, 100)           4100      \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 30, 50)            200050    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 15, 50)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 15, 100)           100100    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 7, 100)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 7, 100)            400       \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 7, 100)            200100    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 3, 100)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 3, 100)            400       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 3, 100)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 15)                4515      \n",
      "=================================================================\n",
      "Total params: 509,665\n",
      "Trainable params: 509,265\n",
      "Non-trainable params: 400\n",
      "_________________________________________________________________\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 30, 1)]           0         \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 30, 1)             0         \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, 15)                509665    \n",
      "_________________________________________________________________\n",
      "feature (Lambda)             (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 15)                240       \n",
      "=================================================================\n",
      "Total params: 509,905\n",
      "Trainable params: 509,505\n",
      "Non-trainable params: 400\n",
      "_________________________________________________________________\n",
      "Epoch 1/400\n",
      "78/78 [==============================] - ETA: 0s - loss: 2.5513\n",
      "Epoch 00001: val_loss improved from inf to 2.77877, saving model to ../model_files/base_cnn_60_seconds_ppg_hr_wesad_dalia_combined_only_classification1.hdf5\n",
      "78/78 [==============================] - 1s 15ms/step - loss: 2.5513 - val_loss: 2.7788\n",
      "Epoch 2/400\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 2.4049\n",
      "Epoch 00002: val_loss improved from 2.77877 to 2.50688, saving model to ../model_files/base_cnn_60_seconds_ppg_hr_wesad_dalia_combined_only_classification1.hdf5\n",
      "78/78 [==============================] - 1s 11ms/step - loss: 2.4017 - val_loss: 2.5069\n",
      "Epoch 3/400\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 2.3028\n",
      "Epoch 00003: val_loss improved from 2.50688 to 2.40433, saving model to ../model_files/base_cnn_60_seconds_ppg_hr_wesad_dalia_combined_only_classification1.hdf5\n",
      "78/78 [==============================] - 1s 11ms/step - loss: 2.3000 - val_loss: 2.4043\n",
      "Epoch 4/400\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 2.2151\n",
      "Epoch 00004: val_loss improved from 2.40433 to 2.38597, saving model to ../model_files/base_cnn_60_seconds_ppg_hr_wesad_dalia_combined_only_classification1.hdf5\n",
      "78/78 [==============================] - 1s 11ms/step - loss: 2.2121 - val_loss: 2.3860\n",
      "Epoch 5/400\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 2.1228\n",
      "Epoch 00005: val_loss did not improve from 2.38597\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 2.1211 - val_loss: 2.4394\n",
      "Epoch 6/400\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 2.0236\n",
      "Epoch 00006: val_loss improved from 2.38597 to 2.37621, saving model to ../model_files/base_cnn_60_seconds_ppg_hr_wesad_dalia_combined_only_classification1.hdf5\n",
      "78/78 [==============================] - 1s 11ms/step - loss: 2.0212 - val_loss: 2.3762\n",
      "Epoch 7/400\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 1.9250\n",
      "Epoch 00007: val_loss did not improve from 2.37621\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 1.9225 - val_loss: 2.4642\n",
      "Epoch 8/400\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 1.8172\n",
      "Epoch 00008: val_loss did not improve from 2.37621\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 1.8157 - val_loss: 2.4485\n",
      "Epoch 9/400\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 1.6998\n",
      "Epoch 00009: val_loss did not improve from 2.37621\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 1.6974 - val_loss: 2.4660\n",
      "Epoch 10/400\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 1.5855\n",
      "Epoch 00010: val_loss did not improve from 2.37621\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 1.5832 - val_loss: 2.4530\n",
      "Epoch 11/400\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 1.4674\n",
      "Epoch 00011: val_loss did not improve from 2.37621\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 1.4645 - val_loss: 2.5451\n",
      "Epoch 12/400\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 1.3456\n",
      "Epoch 00012: val_loss did not improve from 2.37621\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 1.3443 - val_loss: 2.5900\n",
      "Epoch 13/400\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 1.2544\n",
      "Epoch 00013: val_loss did not improve from 2.37621\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 1.2512 - val_loss: 2.6019\n",
      "Epoch 14/400\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 1.1405\n",
      "Epoch 00014: val_loss did not improve from 2.37621\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 1.1410 - val_loss: 2.6902\n",
      "Epoch 15/400\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 1.0725\n",
      "Epoch 00015: val_loss did not improve from 2.37621\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 1.0684 - val_loss: 2.7258\n",
      "Epoch 16/400\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.9432\n",
      "Epoch 00016: val_loss did not improve from 2.37621\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.9426 - val_loss: 2.6757\n",
      "Epoch 17/400\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.8853\n",
      "Epoch 00017: val_loss did not improve from 2.37621\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.8825 - val_loss: 2.8786\n",
      "Epoch 18/400\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.7996\n",
      "Epoch 00018: val_loss did not improve from 2.37621\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.7994 - val_loss: 2.9221\n",
      "Epoch 19/400\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.7333\n",
      "Epoch 00019: val_loss did not improve from 2.37621\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.7308 - val_loss: 3.1161\n",
      "Epoch 20/400\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.6766\n",
      "Epoch 00020: val_loss did not improve from 2.37621\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.6741 - val_loss: 3.0320\n",
      "Epoch 21/400\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.6267\n",
      "Epoch 00021: val_loss did not improve from 2.37621\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.6262 - val_loss: 3.0537\n",
      "Epoch 22/400\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.6195\n",
      "Epoch 00022: val_loss did not improve from 2.37621\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.6179 - val_loss: 3.1455\n",
      "Epoch 23/400\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.5264\n",
      "Epoch 00023: val_loss did not improve from 2.37621\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.5262 - val_loss: 3.2362\n",
      "Epoch 24/400\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.4906\n",
      "Epoch 00024: val_loss did not improve from 2.37621\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.4905 - val_loss: 3.3477\n",
      "Epoch 25/400\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.4637\n",
      "Epoch 00025: val_loss did not improve from 2.37621\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.4622 - val_loss: 3.3551\n",
      "Epoch 26/400\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.4033\n",
      "Epoch 00026: val_loss did not improve from 2.37621\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.4034 - val_loss: 3.4326\n",
      "Epoch 27/400\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.4163\n",
      "Epoch 00027: val_loss did not improve from 2.37621\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.4148 - val_loss: 3.5598\n",
      "Epoch 28/400\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.3589\n",
      "Epoch 00028: val_loss did not improve from 2.37621\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.3596 - val_loss: 3.5118\n",
      "Epoch 29/400\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.3609\n",
      "Epoch 00029: val_loss did not improve from 2.37621\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.3602 - val_loss: 3.5639\n",
      "Epoch 30/400\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.3151\n",
      "Epoch 00030: val_loss did not improve from 2.37621\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.3136 - val_loss: 3.7389\n",
      "Epoch 31/400\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.3323\n",
      "Epoch 00031: val_loss did not improve from 2.37621\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.3307 - val_loss: 3.6640\n",
      "Epoch 32/400\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.3271\n",
      "Epoch 00032: val_loss did not improve from 2.37621\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.3259 - val_loss: 3.7736\n",
      "Epoch 33/400\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.2787\n",
      "Epoch 00033: val_loss did not improve from 2.37621\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.2778 - val_loss: 3.7596\n",
      "Epoch 34/400\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.2693\n",
      "Epoch 00034: val_loss did not improve from 2.37621\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.2685 - val_loss: 3.8346\n",
      "Epoch 35/400\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.2504\n",
      "Epoch 00035: val_loss did not improve from 2.37621\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.2489 - val_loss: 3.8094\n",
      "Epoch 36/400\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.2185\n",
      "Epoch 00036: val_loss did not improve from 2.37621\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.2186 - val_loss: 3.9678\n",
      "Epoch 37/400\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.3625\n",
      "Epoch 00037: val_loss did not improve from 2.37621\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.3563 - val_loss: 3.8773\n",
      "Epoch 38/400\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.2340\n",
      "Epoch 00038: val_loss did not improve from 2.37621\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.2332 - val_loss: 3.9528\n",
      "Epoch 39/400\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.2062\n",
      "Epoch 00039: val_loss did not improve from 2.37621\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.2058 - val_loss: 4.0413\n",
      "Epoch 40/400\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.1973\n",
      "Epoch 00040: val_loss did not improve from 2.37621\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.1963 - val_loss: 4.1218\n",
      "Epoch 41/400\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.2250\n",
      "Epoch 00041: val_loss did not improve from 2.37621\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.2237 - val_loss: 4.1564\n",
      "Epoch 42/400\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.2205\n",
      "Epoch 00042: val_loss did not improve from 2.37621\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.2187 - val_loss: 4.2081\n",
      "Epoch 43/400\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.1796\n",
      "Epoch 00043: val_loss did not improve from 2.37621\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.1794 - val_loss: 4.2248\n",
      "Epoch 44/400\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.1907\n",
      "Epoch 00044: val_loss did not improve from 2.37621\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.1904 - val_loss: 4.1544\n",
      "Epoch 45/400\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.1636\n",
      "Epoch 00045: val_loss did not improve from 2.37621\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.1633 - val_loss: 4.2651\n",
      "Epoch 46/400\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.1658\n",
      "Epoch 00046: val_loss did not improve from 2.37621\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.1650 - val_loss: 4.3099\n",
      "Epoch 00046: early stopping\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "for k in [0]:\n",
    "    n_classes = 200\n",
    "    n_output = len(np.unique(y_participant))\n",
    "    model = get_model(input_shape=(30,1),n_classes=n_output,n_output=n_output) \n",
    "    model.summary()\n",
    "    from keras.models import load_model\n",
    "    filepath = '../model_files/base_cnn_60_seconds_ppg_hr_wesad_dalia_combined_only_classification1.hdf5'\n",
    "\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min',save_weights_only=True)\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=40)\n",
    "    callbacks_list = [es,checkpoint]\n",
    "    history = model.fit(train_x,train_y,validation_data=(val_x,val_y), epochs=400, \n",
    "                        batch_size=500,callbacks=callbacks_list,shuffle=True,verbose=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.15034747432838916\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.07      0.08      0.07      1348\n",
      "           1       0.29      0.17      0.21      1363\n",
      "           2       0.01      0.00      0.00      1181\n",
      "           3       0.09      0.02      0.03      1302\n",
      "           4       0.14      0.36      0.20      1392\n",
      "           5       0.27      0.21      0.24      1178\n",
      "           6       0.00      0.00      0.00       778\n",
      "           7       0.08      0.05      0.06      1361\n",
      "           8       0.20      0.12      0.15      1588\n",
      "           9       0.20      0.51      0.29      1386\n",
      "          10       0.15      0.29      0.20      1275\n",
      "          11       0.15      0.02      0.03      1203\n",
      "          12       0.22      0.04      0.07      1221\n",
      "          13       0.09      0.05      0.06      1334\n",
      "          14       0.13      0.23      0.17      1372\n",
      "\n",
      "    accuracy                           0.15     19282\n",
      "   macro avg       0.14      0.14      0.12     19282\n",
      "weighted avg       0.14      0.15      0.12     19282\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(filepath)\n",
    "test_y_pred = model.predict(test_x).argmax(axis=1)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\n",
    "print(k,accuracy_score(test_y,test_y_pred))\n",
    "print(classification_report(test_y,test_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "result= np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams.update({'font.size':25})\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(result[:,0],result[:,1],'o--r')\n",
    "plt.xlabel('Winsorization Limit')\n",
    "plt.ylabel('Reidentification Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "train_embeddings_pca = TSNE(n_components=2).fit_transform(test_embeddings)\n",
    "plt.figure()\n",
    "plt.scatter(train_embeddings_pca[:,0],train_embeddings_pca[:,1],c=test_y)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "test_dalia_x,test_dalia_y_stress,test_dalia_y,test_dalia_time = pickle.load(open('../data/tabular_data_60_seconds_ppg_rr_dalia_normalized.p',\n",
    "                                                                                 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dalia_x = test_dalia_x.reshape(-1,30,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dalia_embeddings = model.predict(test_dalia_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings_pca = TSNE(n_components=2).fit_transform(test_dalia_embeddings)\n",
    "plt.figure()\n",
    "plt.scatter(train_embeddings_pca[:,0],train_embeddings_pca[:,1],c=test_dalia_y)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(input_shape=(30,1),n_classes=n_classes,n_output=n_output,loss=custom_loss) \n",
    "model.summary()\n",
    "model.load_weights(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(test_x).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(test_y,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def get_clusters(model,train_x,train_y):\n",
    "    train_embeddings = model.predict(train_x)\n",
    "\n",
    "    from sklearn.decomposition import PCA\n",
    "\n",
    "    train_embeddings_pca = PCA(n_components=20).fit_transform(train_embeddings)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(train_embeddings_pca[:,0],train_embeddings_pca[:,1],c=train_y)\n",
    "    plt.show()\n",
    "get_clusters(model,test_x,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "test_dalia_x,test_dalia_y_stress,test_dalia_y,test_dalia_time = pickle.load(open('../data/tabular_data_60_seconds_ppg_rr_dalia.p',\n",
    "                                                                                 'rb'))\n",
    "test_dalia_x = test_dalia_x.reshape(-1,30,1)\n",
    "get_clusters(model,test_dalia_x,test_dalia_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_rank_accuracy(train_embeddings,train_y,rank = 10):\n",
    "    distance_matrix = euclidean_distances(train_embeddings)\n",
    "    distance_matrix_2d = np.zeros((distance_matrix.shape[0],\n",
    "                               distance_matrix.shape[0],\n",
    "                               2))\n",
    "    distance_matrix_2d[:,:,0] = distance_matrix\n",
    "    train_y_matrix = np.concatenate([train_y.reshape(1,-1)]*distance_matrix.shape[0])\n",
    "    distance_matrix_2d[:,:,1] = train_y_matrix\n",
    "    match = 0\n",
    "    for i in range(distance_matrix.shape[0]):\n",
    "        row = distance_matrix_2d[i]\n",
    "        row = row[row[:,0].argsort(),:]\n",
    "        if rank==1:\n",
    "            if train_y[i]==row[1,1]:\n",
    "                match+=1\n",
    "        else:\n",
    "            row = set(row[1:(rank+1)][:,1])\n",
    "            if train_y[i] in row:\n",
    "                match+=1\n",
    "    return match/distance_matrix.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_1 = give_rank_accuracy(train_embeddings,train_y,rank = 1)\n",
    "rank_5 = give_rank_accuracy(train_embeddings,train_y,rank = 5)\n",
    "rank_10 = give_rank_accuracy(train_embeddings,train_y,rank = 10)\n",
    "print(rank_1,rank_5,rank_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embeddings = model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_1 = give_rank_accuracy(test_embeddings,test_y,rank = 1)\n",
    "rank_5 = give_rank_accuracy(test_embeddings,test_y,rank = 5)\n",
    "rank_10 = give_rank_accuracy(test_embeddings,test_y,rank = 10)\n",
    "print(rank_1,rank_5,rank_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x,y_stress,test_y,X_time = pickle.load(open('../data/tabular_data_60_seconds_ppg_rr_dalia.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = test_x.reshape(-1,30,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embeddings = model.predict(test_x)\n",
    "rank_1 = give_rank_accuracy(test_embeddings,test_y,rank = 1)\n",
    "rank_5 = give_rank_accuracy(test_embeddings,test_y,rank = 5)\n",
    "rank_10 = give_rank_accuracy(test_embeddings,test_y,rank = 10)\n",
    "print(rank_1,rank_5,rank_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(train_y,train_y_pred))\n",
    "print(confusion_matrix(train_y,train_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.layers.GaussianNoise(tf.random.uniform((30),minval=10,maxval=11))(test_x[0]),test_x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel,delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
