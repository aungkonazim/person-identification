{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "X_acl,X_ppg,y,y_participant,X_time = pickle.load(open('../data/tabular_data.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_ppg = X_ppg[:,np.arange(0,512,2),:]\n",
    "X_all = np.concatenate([X_ppg,X_acl],axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43363, 256, 8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43363, 256, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all = X_all[:,:,np.array([0,2,3,4])]\n",
    "X_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split,LeaveOneGroupOut,LeavePGroupsOut\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K \n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "%matplotlib notebook\n",
    "from keras.layers import Conv1D,Attention,Reshape,BatchNormalization,TimeDistributed, \\\n",
    "Dropout,Input,MaxPooling1D,Flatten,Dense,Input, GaussianNoise,LSTM, Bidirectional, Input\n",
    "from keras.models import Model, Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37544, 256, 4), (37544,), (5819,), (5819, 256, 4))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logo = LeaveOneGroupOut()\n",
    "logo = LeavePGroupsOut(n_groups=2)\n",
    "for train_index, test_index in logo.split(X_all, y, y_participant.reshape(-1)):\n",
    "    train_x, test_x_unseen = X_all[train_index], X_all[test_index]\n",
    "    train_y, test_y_unseen = y[train_index], y[test_index]\n",
    "    train_participant, test_participant_unseen = y_participant[train_index], y_participant[test_index]\n",
    "    break\n",
    "train_x.shape,train_y.shape,test_y_unseen.shape,test_x_unseen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelBinarizer,LabelEncoder\n",
    "\n",
    "participant_ids = train_participant.copy()\n",
    "train_participant = OneHotEncoder().fit_transform(train_participant.reshape(-1,1)).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y,train_participant, test_participant,participant_ids_train, participant_ids_test = train_test_split(train_x,\n",
    "                                                            train_y,\n",
    "                                                            train_participant,\n",
    "                                                            participant_ids,\n",
    "                                                            test_size = 0.2,\n",
    "                                                            random_state=42,\n",
    "                                                            stratify=participant_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, val_x, train_y, val_y, train_participant,val_participant,participant_ids_train, participant_ids_val  = train_test_split(train_x,\n",
    "                                                            train_y,\n",
    "                                                            train_participant,\n",
    "                                                            participant_ids_train,\n",
    "                                                            test_size = 0.1,\n",
    "                                                            random_state=42,\n",
    "                                                            stratify=participant_ids_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((27031, 256, 4),\n",
       " (7509, 256, 4),\n",
       " (3004, 256, 4),\n",
       " (27031, 13),\n",
       " (7509, 13),\n",
       " (3004, 13))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape,test_x.shape,val_x.shape,train_participant.shape,test_participant.shape,val_participant.shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_self_attention import SeqSelfAttention\n",
    "from keras import backend as K\n",
    "from keras.layers import Concatenate, Flatten,LeakyReLU, Activation, RepeatVector, Permute, Multiply, Lambda, Dense, merge# Define a regular layer instead of writing a custom layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_shape=(256,1),act='relu',loss=\"categorical_crossentropy\",opt='adam',n_classes=350):\n",
    "    model =  Sequential()\n",
    "    model.add(Conv1D(100,10,input_shape=input_shape,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(.2))\n",
    "    model.add(Conv1D(100,10,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(.2))\n",
    "    model.add(Conv1D(100,10,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(50,2,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(n_classes,activation='sigmoid',kernel_initializer='normal'))\n",
    "    model.add(Dense(n_classes,activation='softmax',kernel_initializer='normal'))\n",
    "    model.compile(loss=loss,optimizer=opt,metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 256, 100)          4100      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 128, 100)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 128, 100)          400       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128, 100)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 128, 100)          100100    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 64, 100)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 64, 100)           400       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64, 100)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 64, 100)           100100    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 32, 100)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 100)           400       \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 32, 50)            10050     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 16, 50)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 50)            200       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16, 50)            0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 13)                10413     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 13)                182       \n",
      "=================================================================\n",
      "Total params: 226,345\n",
      "Trainable params: 225,645\n",
      "Non-trainable params: 700\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_classes = len(np.unique(participant_ids_train))\n",
    "model = get_model(input_shape=(256,4),n_classes=n_classes)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "filepath = '../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5'\n",
    "# model = load_model(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 2.3431 - acc: 0.2637\n",
      "Epoch 00001: val_loss improved from inf to 2.17882, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 15ms/step - loss: 2.3431 - acc: 0.2637 - val_loss: 2.1788 - val_acc: 0.3123\n",
      "Epoch 2/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 2.0039 - acc: 0.3925\n",
      "Epoch 00002: val_loss improved from 2.17882 to 1.89207, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 2.0039 - acc: 0.3925 - val_loss: 1.8921 - val_acc: 0.4008\n",
      "Epoch 3/400\n",
      "135/136 [============================>.] - ETA: 0s - loss: 1.7991 - acc: 0.4437\n",
      "Epoch 00003: val_loss improved from 1.89207 to 1.72436, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 1.7990 - acc: 0.4437 - val_loss: 1.7244 - val_acc: 0.4757\n",
      "Epoch 4/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 1.6397 - acc: 0.5021\n",
      "Epoch 00004: val_loss improved from 1.72436 to 1.57777, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 1.6397 - acc: 0.5021 - val_loss: 1.5778 - val_acc: 0.5170\n",
      "Epoch 5/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 1.4953 - acc: 0.5608\n",
      "Epoch 00005: val_loss improved from 1.57777 to 1.43722, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 1.4953 - acc: 0.5608 - val_loss: 1.4372 - val_acc: 0.5789\n",
      "Epoch 6/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 1.3607 - acc: 0.6113\n",
      "Epoch 00006: val_loss improved from 1.43722 to 1.30131, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 1.3607 - acc: 0.6113 - val_loss: 1.3013 - val_acc: 0.6378\n",
      "Epoch 7/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 1.2179 - acc: 0.6583\n",
      "Epoch 00007: val_loss improved from 1.30131 to 1.15965, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 1.2179 - acc: 0.6583 - val_loss: 1.1596 - val_acc: 0.6734\n",
      "Epoch 8/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 1.1073 - acc: 0.6811\n",
      "Epoch 00008: val_loss improved from 1.15965 to 1.11388, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 1.1073 - acc: 0.6811 - val_loss: 1.1139 - val_acc: 0.6548\n",
      "Epoch 9/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 1.0028 - acc: 0.7087\n",
      "Epoch 00009: val_loss improved from 1.11388 to 1.00438, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 1.0028 - acc: 0.7087 - val_loss: 1.0044 - val_acc: 0.7021\n",
      "Epoch 10/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.9089 - acc: 0.7306\n",
      "Epoch 00010: val_loss improved from 1.00438 to 0.89212, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.9089 - acc: 0.7306 - val_loss: 0.8921 - val_acc: 0.7324\n",
      "Epoch 11/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.8356 - acc: 0.7480\n",
      "Epoch 00011: val_loss improved from 0.89212 to 0.83075, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.8356 - acc: 0.7480 - val_loss: 0.8307 - val_acc: 0.7450\n",
      "Epoch 12/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.7759 - acc: 0.7599\n",
      "Epoch 00012: val_loss improved from 0.83075 to 0.78740, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.7759 - acc: 0.7599 - val_loss: 0.7874 - val_acc: 0.7493\n",
      "Epoch 13/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.7188 - acc: 0.7705\n",
      "Epoch 00013: val_loss improved from 0.78740 to 0.73898, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.7188 - acc: 0.7705 - val_loss: 0.7390 - val_acc: 0.7527\n",
      "Epoch 14/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.6624 - acc: 0.7835\n",
      "Epoch 00014: val_loss improved from 0.73898 to 0.67953, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.6624 - acc: 0.7835 - val_loss: 0.6795 - val_acc: 0.7790\n",
      "Epoch 15/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.6053 - acc: 0.7974\n",
      "Epoch 00015: val_loss improved from 0.67953 to 0.60246, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.6053 - acc: 0.7974 - val_loss: 0.6025 - val_acc: 0.7919\n",
      "Epoch 16/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.5650 - acc: 0.8180\n",
      "Epoch 00016: val_loss did not improve from 0.60246\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.5650 - acc: 0.8180 - val_loss: 0.6146 - val_acc: 0.8186\n",
      "Epoch 17/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.5089 - acc: 0.8534\n",
      "Epoch 00017: val_loss improved from 0.60246 to 0.60010, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.5089 - acc: 0.8534 - val_loss: 0.6001 - val_acc: 0.8239\n",
      "Epoch 18/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.4675 - acc: 0.8691\n",
      "Epoch 00018: val_loss improved from 0.60010 to 0.49533, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.4675 - acc: 0.8691 - val_loss: 0.4953 - val_acc: 0.8615\n",
      "Epoch 19/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.4240 - acc: 0.8829\n",
      "Epoch 00019: val_loss improved from 0.49533 to 0.46133, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.4240 - acc: 0.8829 - val_loss: 0.4613 - val_acc: 0.8662\n",
      "Epoch 20/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.3894 - acc: 0.8914\n",
      "Epoch 00020: val_loss improved from 0.46133 to 0.43525, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.3894 - acc: 0.8914 - val_loss: 0.4353 - val_acc: 0.8748\n",
      "Epoch 21/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.3596 - acc: 0.8997\n",
      "Epoch 00021: val_loss improved from 0.43525 to 0.40023, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.3596 - acc: 0.8997 - val_loss: 0.4002 - val_acc: 0.8848\n",
      "Epoch 22/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.3306 - acc: 0.9078\n",
      "Epoch 00022: val_loss improved from 0.40023 to 0.36825, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.3306 - acc: 0.9078 - val_loss: 0.3682 - val_acc: 0.8901\n",
      "Epoch 23/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.3074 - acc: 0.9143\n",
      "Epoch 00023: val_loss improved from 0.36825 to 0.36109, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.3074 - acc: 0.9143 - val_loss: 0.3611 - val_acc: 0.8955\n",
      "Epoch 24/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.2919 - acc: 0.9162\n",
      "Epoch 00024: val_loss improved from 0.36109 to 0.31721, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.2919 - acc: 0.9162 - val_loss: 0.3172 - val_acc: 0.9091\n",
      "Epoch 25/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.2788 - acc: 0.9202\n",
      "Epoch 00025: val_loss did not improve from 0.31721\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.2788 - acc: 0.9202 - val_loss: 0.3180 - val_acc: 0.9061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.2649 - acc: 0.9238\n",
      "Epoch 00026: val_loss improved from 0.31721 to 0.28633, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.2649 - acc: 0.9238 - val_loss: 0.2863 - val_acc: 0.9154\n",
      "Epoch 27/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.2581 - acc: 0.9241\n",
      "Epoch 00027: val_loss did not improve from 0.28633\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.2581 - acc: 0.9241 - val_loss: 0.3201 - val_acc: 0.9068\n",
      "Epoch 28/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.2364 - acc: 0.9324\n",
      "Epoch 00028: val_loss improved from 0.28633 to 0.25973, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.2364 - acc: 0.9324 - val_loss: 0.2597 - val_acc: 0.9238\n",
      "Epoch 29/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.2160 - acc: 0.9377\n",
      "Epoch 00029: val_loss did not improve from 0.25973\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.2160 - acc: 0.9377 - val_loss: 0.2732 - val_acc: 0.9198\n",
      "Epoch 30/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.2165 - acc: 0.9374\n",
      "Epoch 00030: val_loss did not improve from 0.25973\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.2165 - acc: 0.9374 - val_loss: 0.2743 - val_acc: 0.9191\n",
      "Epoch 31/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.2090 - acc: 0.9383\n",
      "Epoch 00031: val_loss improved from 0.25973 to 0.25069, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.2090 - acc: 0.9383 - val_loss: 0.2507 - val_acc: 0.9258\n",
      "Epoch 32/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.2082 - acc: 0.9390\n",
      "Epoch 00032: val_loss improved from 0.25069 to 0.21452, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.2082 - acc: 0.9390 - val_loss: 0.2145 - val_acc: 0.9381\n",
      "Epoch 33/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.1845 - acc: 0.9464\n",
      "Epoch 00033: val_loss did not improve from 0.21452\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.1845 - acc: 0.9464 - val_loss: 0.2377 - val_acc: 0.9278\n",
      "Epoch 34/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.1842 - acc: 0.9460\n",
      "Epoch 00034: val_loss improved from 0.21452 to 0.21234, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.1842 - acc: 0.9460 - val_loss: 0.2123 - val_acc: 0.9407\n",
      "Epoch 35/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.1776 - acc: 0.9485\n",
      "Epoch 00035: val_loss did not improve from 0.21234\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.1776 - acc: 0.9485 - val_loss: 0.2228 - val_acc: 0.9324\n",
      "Epoch 36/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.1753 - acc: 0.9495\n",
      "Epoch 00036: val_loss did not improve from 0.21234\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.1753 - acc: 0.9495 - val_loss: 0.2285 - val_acc: 0.9288\n",
      "Epoch 37/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.1644 - acc: 0.9506\n",
      "Epoch 00037: val_loss improved from 0.21234 to 0.21064, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.1644 - acc: 0.9506 - val_loss: 0.2106 - val_acc: 0.9341\n",
      "Epoch 38/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.1591 - acc: 0.9534\n",
      "Epoch 00038: val_loss did not improve from 0.21064\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.1591 - acc: 0.9534 - val_loss: 0.2172 - val_acc: 0.9341\n",
      "Epoch 39/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.1624 - acc: 0.9520\n",
      "Epoch 00039: val_loss improved from 0.21064 to 0.19695, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.1624 - acc: 0.9520 - val_loss: 0.1970 - val_acc: 0.9401\n",
      "Epoch 40/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.1486 - acc: 0.9547\n",
      "Epoch 00040: val_loss did not improve from 0.19695\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.1486 - acc: 0.9547 - val_loss: 0.2057 - val_acc: 0.9361\n",
      "Epoch 41/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.1409 - acc: 0.9589\n",
      "Epoch 00041: val_loss improved from 0.19695 to 0.18912, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.1409 - acc: 0.9589 - val_loss: 0.1891 - val_acc: 0.9437\n",
      "Epoch 42/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.1336 - acc: 0.9606\n",
      "Epoch 00042: val_loss did not improve from 0.18912\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.1336 - acc: 0.9606 - val_loss: 0.2222 - val_acc: 0.9284\n",
      "Epoch 43/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.1414 - acc: 0.9573\n",
      "Epoch 00043: val_loss did not improve from 0.18912\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.1414 - acc: 0.9573 - val_loss: 0.2087 - val_acc: 0.9404\n",
      "Epoch 44/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.1540 - acc: 0.9519\n",
      "Epoch 00044: val_loss did not improve from 0.18912\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.1540 - acc: 0.9519 - val_loss: 0.1977 - val_acc: 0.9447\n",
      "Epoch 45/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.1272 - acc: 0.9612\n",
      "Epoch 00045: val_loss improved from 0.18912 to 0.17949, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.1272 - acc: 0.9612 - val_loss: 0.1795 - val_acc: 0.9504\n",
      "Epoch 46/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.1285 - acc: 0.9607\n",
      "Epoch 00046: val_loss did not improve from 0.17949\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.1285 - acc: 0.9607 - val_loss: 0.3076 - val_acc: 0.9045\n",
      "Epoch 47/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.1511 - acc: 0.9521\n",
      "Epoch 00047: val_loss improved from 0.17949 to 0.16319, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.1511 - acc: 0.9521 - val_loss: 0.1632 - val_acc: 0.9487\n",
      "Epoch 48/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.1287 - acc: 0.9604\n",
      "Epoch 00048: val_loss did not improve from 0.16319\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.1287 - acc: 0.9604 - val_loss: 0.1826 - val_acc: 0.9407\n",
      "Epoch 49/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.1170 - acc: 0.9647\n",
      "Epoch 00049: val_loss did not improve from 0.16319\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.1170 - acc: 0.9647 - val_loss: 0.1689 - val_acc: 0.9504\n",
      "Epoch 50/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.1180 - acc: 0.9630\n",
      "Epoch 00050: val_loss did not improve from 0.16319\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.1180 - acc: 0.9630 - val_loss: 0.1781 - val_acc: 0.9484\n",
      "Epoch 51/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.1170 - acc: 0.9650\n",
      "Epoch 00051: val_loss did not improve from 0.16319\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.1170 - acc: 0.9650 - val_loss: 0.1657 - val_acc: 0.9487\n",
      "Epoch 52/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.1127 - acc: 0.9649\n",
      "Epoch 00052: val_loss did not improve from 0.16319\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.1127 - acc: 0.9649 - val_loss: 0.1881 - val_acc: 0.9461\n",
      "Epoch 53/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.1023 - acc: 0.9687\n",
      "Epoch 00053: val_loss did not improve from 0.16319\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.1023 - acc: 0.9687 - val_loss: 0.1719 - val_acc: 0.9481\n",
      "Epoch 54/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - ETA: 0s - loss: 0.1077 - acc: 0.9660\n",
      "Epoch 00054: val_loss improved from 0.16319 to 0.16001, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.1077 - acc: 0.9660 - val_loss: 0.1600 - val_acc: 0.9511\n",
      "Epoch 55/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.1080 - acc: 0.9674\n",
      "Epoch 00055: val_loss improved from 0.16001 to 0.15247, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.1080 - acc: 0.9674 - val_loss: 0.1525 - val_acc: 0.9537\n",
      "Epoch 56/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.1400 - acc: 0.9559\n",
      "Epoch 00056: val_loss did not improve from 0.15247\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.1400 - acc: 0.9559 - val_loss: 0.2151 - val_acc: 0.9341\n",
      "Epoch 57/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.1099 - acc: 0.9669\n",
      "Epoch 00057: val_loss did not improve from 0.15247\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.1099 - acc: 0.9669 - val_loss: 0.1532 - val_acc: 0.9591\n",
      "Epoch 58/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.1152 - acc: 0.9632\n",
      "Epoch 00058: val_loss did not improve from 0.15247\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.1152 - acc: 0.9632 - val_loss: 0.1591 - val_acc: 0.9534\n",
      "Epoch 59/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.1109 - acc: 0.9662\n",
      "Epoch 00059: val_loss did not improve from 0.15247\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.1109 - acc: 0.9662 - val_loss: 0.1593 - val_acc: 0.9521\n",
      "Epoch 60/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0992 - acc: 0.9697\n",
      "Epoch 00060: val_loss did not improve from 0.15247\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0992 - acc: 0.9697 - val_loss: 0.1621 - val_acc: 0.9517\n",
      "Epoch 61/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0930 - acc: 0.9714\n",
      "Epoch 00061: val_loss improved from 0.15247 to 0.14587, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.0930 - acc: 0.9714 - val_loss: 0.1459 - val_acc: 0.9574\n",
      "Epoch 62/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0957 - acc: 0.9709\n",
      "Epoch 00062: val_loss did not improve from 0.14587\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0957 - acc: 0.9709 - val_loss: 0.1614 - val_acc: 0.9464\n",
      "Epoch 63/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.1013 - acc: 0.9681\n",
      "Epoch 00063: val_loss improved from 0.14587 to 0.12457, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.1013 - acc: 0.9681 - val_loss: 0.1246 - val_acc: 0.9644\n",
      "Epoch 64/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.1005 - acc: 0.9684\n",
      "Epoch 00064: val_loss did not improve from 0.12457\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.1005 - acc: 0.9684 - val_loss: 0.1694 - val_acc: 0.9474\n",
      "Epoch 65/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0998 - acc: 0.9697\n",
      "Epoch 00065: val_loss did not improve from 0.12457\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0998 - acc: 0.9697 - val_loss: 0.1325 - val_acc: 0.9601\n",
      "Epoch 66/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0916 - acc: 0.9725\n",
      "Epoch 00066: val_loss did not improve from 0.12457\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0916 - acc: 0.9725 - val_loss: 0.1829 - val_acc: 0.9477\n",
      "Epoch 67/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0833 - acc: 0.9746\n",
      "Epoch 00067: val_loss did not improve from 0.12457\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0833 - acc: 0.9746 - val_loss: 0.1411 - val_acc: 0.9571\n",
      "Epoch 68/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0802 - acc: 0.9758\n",
      "Epoch 00068: val_loss did not improve from 0.12457\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0802 - acc: 0.9758 - val_loss: 0.1433 - val_acc: 0.9541\n",
      "Epoch 69/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0797 - acc: 0.9743\n",
      "Epoch 00069: val_loss did not improve from 0.12457\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0797 - acc: 0.9743 - val_loss: 0.1353 - val_acc: 0.9581\n",
      "Epoch 70/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0838 - acc: 0.9751\n",
      "Epoch 00070: val_loss did not improve from 0.12457\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0838 - acc: 0.9751 - val_loss: 0.1397 - val_acc: 0.9571\n",
      "Epoch 71/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0814 - acc: 0.9749\n",
      "Epoch 00071: val_loss did not improve from 0.12457\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0814 - acc: 0.9749 - val_loss: 0.1434 - val_acc: 0.9554\n",
      "Epoch 72/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0862 - acc: 0.9741\n",
      "Epoch 00072: val_loss did not improve from 0.12457\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0862 - acc: 0.9741 - val_loss: 0.1327 - val_acc: 0.9611\n",
      "Epoch 73/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0837 - acc: 0.9737\n",
      "Epoch 00073: val_loss did not improve from 0.12457\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0837 - acc: 0.9737 - val_loss: 0.1423 - val_acc: 0.9571\n",
      "Epoch 74/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0749 - acc: 0.9770\n",
      "Epoch 00074: val_loss did not improve from 0.12457\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0749 - acc: 0.9770 - val_loss: 0.1435 - val_acc: 0.9547\n",
      "Epoch 75/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0896 - acc: 0.9733\n",
      "Epoch 00075: val_loss improved from 0.12457 to 0.12339, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.0896 - acc: 0.9733 - val_loss: 0.1234 - val_acc: 0.9607\n",
      "Epoch 76/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0964 - acc: 0.9690\n",
      "Epoch 00076: val_loss did not improve from 0.12339\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0964 - acc: 0.9690 - val_loss: 0.1748 - val_acc: 0.9491\n",
      "Epoch 77/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0846 - acc: 0.9743\n",
      "Epoch 00077: val_loss improved from 0.12339 to 0.11681, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.0846 - acc: 0.9743 - val_loss: 0.1168 - val_acc: 0.9657\n",
      "Epoch 78/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0788 - acc: 0.9763\n",
      "Epoch 00078: val_loss did not improve from 0.11681\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0788 - acc: 0.9763 - val_loss: 0.1247 - val_acc: 0.9634\n",
      "Epoch 79/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0805 - acc: 0.9749\n",
      "Epoch 00079: val_loss did not improve from 0.11681\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0805 - acc: 0.9749 - val_loss: 0.1462 - val_acc: 0.9567\n",
      "Epoch 80/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0994 - acc: 0.9680\n",
      "Epoch 00080: val_loss did not improve from 0.11681\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0994 - acc: 0.9680 - val_loss: 0.1264 - val_acc: 0.9591\n",
      "Epoch 81/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0730 - acc: 0.9771\n",
      "Epoch 00081: val_loss did not improve from 0.11681\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0730 - acc: 0.9771 - val_loss: 0.1243 - val_acc: 0.9637\n",
      "Epoch 82/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0736 - acc: 0.9771\n",
      "Epoch 00082: val_loss did not improve from 0.11681\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0736 - acc: 0.9771 - val_loss: 0.1347 - val_acc: 0.9621\n",
      "Epoch 83/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - ETA: 0s - loss: 0.0708 - acc: 0.9779\n",
      "Epoch 00083: val_loss did not improve from 0.11681\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0708 - acc: 0.9779 - val_loss: 0.1269 - val_acc: 0.9604\n",
      "Epoch 84/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0601 - acc: 0.9815\n",
      "Epoch 00084: val_loss improved from 0.11681 to 0.10893, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.0601 - acc: 0.9815 - val_loss: 0.1089 - val_acc: 0.9654\n",
      "Epoch 85/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0709 - acc: 0.9776\n",
      "Epoch 00085: val_loss did not improve from 0.10893\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0709 - acc: 0.9776 - val_loss: 0.1143 - val_acc: 0.9660\n",
      "Epoch 86/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0682 - acc: 0.9785\n",
      "Epoch 00086: val_loss did not improve from 0.10893\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0682 - acc: 0.9785 - val_loss: 0.1361 - val_acc: 0.9627\n",
      "Epoch 87/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0783 - acc: 0.9757\n",
      "Epoch 00087: val_loss did not improve from 0.10893\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0783 - acc: 0.9757 - val_loss: 0.1158 - val_acc: 0.9654\n",
      "Epoch 88/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0802 - acc: 0.9757\n",
      "Epoch 00088: val_loss did not improve from 0.10893\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0802 - acc: 0.9757 - val_loss: 0.1256 - val_acc: 0.9597\n",
      "Epoch 89/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0590 - acc: 0.9823\n",
      "Epoch 00089: val_loss did not improve from 0.10893\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0590 - acc: 0.9823 - val_loss: 0.1274 - val_acc: 0.9607\n",
      "Epoch 90/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0712 - acc: 0.9774\n",
      "Epoch 00090: val_loss did not improve from 0.10893\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0712 - acc: 0.9774 - val_loss: 0.1264 - val_acc: 0.9630\n",
      "Epoch 91/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0872 - acc: 0.9725\n",
      "Epoch 00091: val_loss did not improve from 0.10893\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0872 - acc: 0.9725 - val_loss: 0.1350 - val_acc: 0.9601\n",
      "Epoch 92/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0707 - acc: 0.9775\n",
      "Epoch 00092: val_loss did not improve from 0.10893\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0707 - acc: 0.9775 - val_loss: 0.1247 - val_acc: 0.9637\n",
      "Epoch 93/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0646 - acc: 0.9807\n",
      "Epoch 00093: val_loss did not improve from 0.10893\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0646 - acc: 0.9807 - val_loss: 0.1395 - val_acc: 0.9547\n",
      "Epoch 94/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0721 - acc: 0.9785\n",
      "Epoch 00094: val_loss improved from 0.10893 to 0.10756, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.0721 - acc: 0.9785 - val_loss: 0.1076 - val_acc: 0.9647\n",
      "Epoch 95/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0588 - acc: 0.9820\n",
      "Epoch 00095: val_loss did not improve from 0.10756\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0588 - acc: 0.9820 - val_loss: 0.1469 - val_acc: 0.9551\n",
      "Epoch 96/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0668 - acc: 0.9791\n",
      "Epoch 00096: val_loss did not improve from 0.10756\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0668 - acc: 0.9791 - val_loss: 0.1205 - val_acc: 0.9591\n",
      "Epoch 97/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0614 - acc: 0.9812\n",
      "Epoch 00097: val_loss improved from 0.10756 to 0.10691, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.0614 - acc: 0.9812 - val_loss: 0.1069 - val_acc: 0.9674\n",
      "Epoch 98/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0581 - acc: 0.9821\n",
      "Epoch 00098: val_loss did not improve from 0.10691\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0581 - acc: 0.9821 - val_loss: 0.1255 - val_acc: 0.9597\n",
      "Epoch 99/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0544 - acc: 0.9830\n",
      "Epoch 00099: val_loss did not improve from 0.10691\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0544 - acc: 0.9830 - val_loss: 0.1201 - val_acc: 0.9667\n",
      "Epoch 100/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0585 - acc: 0.9824\n",
      "Epoch 00100: val_loss did not improve from 0.10691\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0585 - acc: 0.9824 - val_loss: 0.1287 - val_acc: 0.9604\n",
      "Epoch 101/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0656 - acc: 0.9792\n",
      "Epoch 00101: val_loss did not improve from 0.10691\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.0656 - acc: 0.9792 - val_loss: 0.1229 - val_acc: 0.9644\n",
      "Epoch 102/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0657 - acc: 0.9799\n",
      "Epoch 00102: val_loss did not improve from 0.10691\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0657 - acc: 0.9799 - val_loss: 0.1070 - val_acc: 0.9687\n",
      "Epoch 103/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0609 - acc: 0.9814\n",
      "Epoch 00103: val_loss improved from 0.10691 to 0.10608, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.0609 - acc: 0.9814 - val_loss: 0.1061 - val_acc: 0.9694\n",
      "Epoch 104/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0621 - acc: 0.9804\n",
      "Epoch 00104: val_loss did not improve from 0.10608\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0621 - acc: 0.9804 - val_loss: 0.1066 - val_acc: 0.9694\n",
      "Epoch 105/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0550 - acc: 0.9829\n",
      "Epoch 00105: val_loss did not improve from 0.10608\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0550 - acc: 0.9829 - val_loss: 0.1097 - val_acc: 0.9684\n",
      "Epoch 106/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0620 - acc: 0.9799\n",
      "Epoch 00106: val_loss did not improve from 0.10608\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0620 - acc: 0.9799 - val_loss: 0.1168 - val_acc: 0.9637\n",
      "Epoch 107/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0533 - acc: 0.9830\n",
      "Epoch 00107: val_loss did not improve from 0.10608\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0533 - acc: 0.9830 - val_loss: 0.1145 - val_acc: 0.9627\n",
      "Epoch 108/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0598 - acc: 0.9807\n",
      "Epoch 00108: val_loss did not improve from 0.10608\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0598 - acc: 0.9807 - val_loss: 0.1125 - val_acc: 0.9670\n",
      "Epoch 109/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0538 - acc: 0.9828\n",
      "Epoch 00109: val_loss did not improve from 0.10608\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0538 - acc: 0.9828 - val_loss: 0.1257 - val_acc: 0.9637\n",
      "Epoch 110/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0596 - acc: 0.9817\n",
      "Epoch 00110: val_loss did not improve from 0.10608\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0596 - acc: 0.9817 - val_loss: 0.1295 - val_acc: 0.9597\n",
      "Epoch 111/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0600 - acc: 0.9811\n",
      "Epoch 00111: val_loss did not improve from 0.10608\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0600 - acc: 0.9811 - val_loss: 0.1095 - val_acc: 0.9657\n",
      "Epoch 112/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0577 - acc: 0.9815\n",
      "Epoch 00112: val_loss improved from 0.10608 to 0.09981, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 2s 13ms/step - loss: 0.0577 - acc: 0.9815 - val_loss: 0.0998 - val_acc: 0.9647\n",
      "Epoch 113/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0521 - acc: 0.9841\n",
      "Epoch 00113: val_loss did not improve from 0.09981\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0521 - acc: 0.9841 - val_loss: 0.1031 - val_acc: 0.9667\n",
      "Epoch 114/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0524 - acc: 0.9841\n",
      "Epoch 00114: val_loss did not improve from 0.09981\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0524 - acc: 0.9841 - val_loss: 0.1035 - val_acc: 0.9704\n",
      "Epoch 115/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0481 - acc: 0.9850\n",
      "Epoch 00115: val_loss improved from 0.09981 to 0.09917, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.0481 - acc: 0.9850 - val_loss: 0.0992 - val_acc: 0.9700\n",
      "Epoch 116/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0469 - acc: 0.9857\n",
      "Epoch 00116: val_loss improved from 0.09917 to 0.09010, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.0469 - acc: 0.9857 - val_loss: 0.0901 - val_acc: 0.9730\n",
      "Epoch 117/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0497 - acc: 0.9841\n",
      "Epoch 00117: val_loss did not improve from 0.09010\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0497 - acc: 0.9841 - val_loss: 0.1063 - val_acc: 0.9697\n",
      "Epoch 118/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0504 - acc: 0.9844\n",
      "Epoch 00118: val_loss did not improve from 0.09010\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0504 - acc: 0.9844 - val_loss: 0.1170 - val_acc: 0.9647\n",
      "Epoch 119/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0600 - acc: 0.9802\n",
      "Epoch 00119: val_loss did not improve from 0.09010\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0600 - acc: 0.9802 - val_loss: 0.1102 - val_acc: 0.9654\n",
      "Epoch 120/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0520 - acc: 0.9835\n",
      "Epoch 00120: val_loss did not improve from 0.09010\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0520 - acc: 0.9835 - val_loss: 0.0983 - val_acc: 0.9710\n",
      "Epoch 121/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0486 - acc: 0.9851\n",
      "Epoch 00121: val_loss did not improve from 0.09010\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0486 - acc: 0.9851 - val_loss: 0.1078 - val_acc: 0.9677\n",
      "Epoch 122/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0647 - acc: 0.9799\n",
      "Epoch 00122: val_loss did not improve from 0.09010\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0647 - acc: 0.9799 - val_loss: 0.0979 - val_acc: 0.9707\n",
      "Epoch 123/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0556 - acc: 0.9819\n",
      "Epoch 00123: val_loss did not improve from 0.09010\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0556 - acc: 0.9819 - val_loss: 0.0969 - val_acc: 0.9724\n",
      "Epoch 124/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0628 - acc: 0.9801\n",
      "Epoch 00124: val_loss did not improve from 0.09010\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0628 - acc: 0.9801 - val_loss: 0.1018 - val_acc: 0.9667\n",
      "Epoch 125/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0496 - acc: 0.9843\n",
      "Epoch 00125: val_loss did not improve from 0.09010\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0496 - acc: 0.9843 - val_loss: 0.1069 - val_acc: 0.9677\n",
      "Epoch 126/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0551 - acc: 0.9832\n",
      "Epoch 00126: val_loss did not improve from 0.09010\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0551 - acc: 0.9832 - val_loss: 0.1032 - val_acc: 0.9724\n",
      "Epoch 127/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.1009 - acc: 0.9669\n",
      "Epoch 00127: val_loss did not improve from 0.09010\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.1009 - acc: 0.9669 - val_loss: 0.1291 - val_acc: 0.9611\n",
      "Epoch 128/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0597 - acc: 0.9817\n",
      "Epoch 00128: val_loss did not improve from 0.09010\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0597 - acc: 0.9817 - val_loss: 0.0953 - val_acc: 0.9710\n",
      "Epoch 129/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0509 - acc: 0.9835\n",
      "Epoch 00129: val_loss improved from 0.09010 to 0.08589, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.0509 - acc: 0.9835 - val_loss: 0.0859 - val_acc: 0.9737\n",
      "Epoch 130/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0492 - acc: 0.9846\n",
      "Epoch 00130: val_loss did not improve from 0.08589\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0492 - acc: 0.9846 - val_loss: 0.0979 - val_acc: 0.9710\n",
      "Epoch 131/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0512 - acc: 0.9836\n",
      "Epoch 00131: val_loss did not improve from 0.08589\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0512 - acc: 0.9836 - val_loss: 0.0864 - val_acc: 0.9777\n",
      "Epoch 132/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0494 - acc: 0.9839\n",
      "Epoch 00132: val_loss improved from 0.08589 to 0.08452, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.0494 - acc: 0.9839 - val_loss: 0.0845 - val_acc: 0.9784\n",
      "Epoch 133/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0406 - acc: 0.9872\n",
      "Epoch 00133: val_loss did not improve from 0.08452\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0406 - acc: 0.9872 - val_loss: 0.0848 - val_acc: 0.9770\n",
      "Epoch 134/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0348 - acc: 0.9896\n",
      "Epoch 00134: val_loss improved from 0.08452 to 0.07897, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.0348 - acc: 0.9896 - val_loss: 0.0790 - val_acc: 0.9780\n",
      "Epoch 135/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0440 - acc: 0.9865\n",
      "Epoch 00135: val_loss did not improve from 0.07897\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0440 - acc: 0.9865 - val_loss: 0.0869 - val_acc: 0.9717\n",
      "Epoch 136/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0457 - acc: 0.9858\n",
      "Epoch 00136: val_loss did not improve from 0.07897\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0457 - acc: 0.9858 - val_loss: 0.0911 - val_acc: 0.9730\n",
      "Epoch 137/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0394 - acc: 0.9876\n",
      "Epoch 00137: val_loss did not improve from 0.07897\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0394 - acc: 0.9876 - val_loss: 0.0820 - val_acc: 0.9754\n",
      "Epoch 138/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0499 - acc: 0.9842\n",
      "Epoch 00138: val_loss did not improve from 0.07897\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0499 - acc: 0.9842 - val_loss: 0.1041 - val_acc: 0.9704\n",
      "Epoch 139/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0528 - acc: 0.9833\n",
      "Epoch 00139: val_loss did not improve from 0.07897\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0528 - acc: 0.9833 - val_loss: 0.0949 - val_acc: 0.9704\n",
      "Epoch 140/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0519 - acc: 0.9842\n",
      "Epoch 00140: val_loss did not improve from 0.07897\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0519 - acc: 0.9842 - val_loss: 0.0909 - val_acc: 0.9724\n",
      "Epoch 141/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0409 - acc: 0.9881\n",
      "Epoch 00141: val_loss did not improve from 0.07897\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0409 - acc: 0.9881 - val_loss: 0.0838 - val_acc: 0.9734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0380 - acc: 0.9882\n",
      "Epoch 00142: val_loss did not improve from 0.07897\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0380 - acc: 0.9882 - val_loss: 0.0807 - val_acc: 0.9750\n",
      "Epoch 143/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0475 - acc: 0.9849\n",
      "Epoch 00143: val_loss did not improve from 0.07897\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0475 - acc: 0.9849 - val_loss: 0.0845 - val_acc: 0.9770\n",
      "Epoch 144/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0463 - acc: 0.9863\n",
      "Epoch 00144: val_loss did not improve from 0.07897\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0463 - acc: 0.9863 - val_loss: 0.0912 - val_acc: 0.9737\n",
      "Epoch 145/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0446 - acc: 0.9854\n",
      "Epoch 00145: val_loss did not improve from 0.07897\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0446 - acc: 0.9854 - val_loss: 0.1010 - val_acc: 0.9694\n",
      "Epoch 146/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0417 - acc: 0.9871\n",
      "Epoch 00146: val_loss did not improve from 0.07897\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0417 - acc: 0.9871 - val_loss: 0.1038 - val_acc: 0.9690\n",
      "Epoch 147/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0369 - acc: 0.9888\n",
      "Epoch 00147: val_loss improved from 0.07897 to 0.06342, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.0369 - acc: 0.9888 - val_loss: 0.0634 - val_acc: 0.9790\n",
      "Epoch 148/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0481 - acc: 0.9855\n",
      "Epoch 00148: val_loss did not improve from 0.06342\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0481 - acc: 0.9855 - val_loss: 0.0833 - val_acc: 0.9740\n",
      "Epoch 149/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0457 - acc: 0.9853\n",
      "Epoch 00149: val_loss did not improve from 0.06342\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0457 - acc: 0.9853 - val_loss: 0.0860 - val_acc: 0.9744\n",
      "Epoch 150/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0436 - acc: 0.9864\n",
      "Epoch 00150: val_loss did not improve from 0.06342\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0436 - acc: 0.9864 - val_loss: 0.0822 - val_acc: 0.9760\n",
      "Epoch 151/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0487 - acc: 0.9849\n",
      "Epoch 00151: val_loss did not improve from 0.06342\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0487 - acc: 0.9849 - val_loss: 0.0891 - val_acc: 0.9740\n",
      "Epoch 152/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0418 - acc: 0.9875\n",
      "Epoch 00152: val_loss did not improve from 0.06342\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0418 - acc: 0.9875 - val_loss: 0.0830 - val_acc: 0.9777\n",
      "Epoch 153/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0402 - acc: 0.9866\n",
      "Epoch 00153: val_loss did not improve from 0.06342\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0402 - acc: 0.9866 - val_loss: 0.0764 - val_acc: 0.9770\n",
      "Epoch 154/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0428 - acc: 0.9859\n",
      "Epoch 00154: val_loss did not improve from 0.06342\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0428 - acc: 0.9859 - val_loss: 0.0907 - val_acc: 0.9737\n",
      "Epoch 155/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0434 - acc: 0.9865\n",
      "Epoch 00155: val_loss did not improve from 0.06342\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0434 - acc: 0.9865 - val_loss: 0.0827 - val_acc: 0.9730\n",
      "Epoch 156/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0392 - acc: 0.9872\n",
      "Epoch 00156: val_loss did not improve from 0.06342\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0392 - acc: 0.9872 - val_loss: 0.0790 - val_acc: 0.9740\n",
      "Epoch 157/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0363 - acc: 0.9885\n",
      "Epoch 00157: val_loss did not improve from 0.06342\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0363 - acc: 0.9885 - val_loss: 0.0813 - val_acc: 0.9737\n",
      "Epoch 158/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0391 - acc: 0.9876\n",
      "Epoch 00158: val_loss did not improve from 0.06342\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0391 - acc: 0.9876 - val_loss: 0.1055 - val_acc: 0.9687\n",
      "Epoch 159/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0450 - acc: 0.9852\n",
      "Epoch 00159: val_loss did not improve from 0.06342\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0450 - acc: 0.9852 - val_loss: 0.0850 - val_acc: 0.9730\n",
      "Epoch 160/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0466 - acc: 0.9858\n",
      "Epoch 00160: val_loss did not improve from 0.06342\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0466 - acc: 0.9858 - val_loss: 0.0869 - val_acc: 0.9740\n",
      "Epoch 161/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0397 - acc: 0.9879\n",
      "Epoch 00161: val_loss did not improve from 0.06342\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0397 - acc: 0.9879 - val_loss: 0.0940 - val_acc: 0.9730\n",
      "Epoch 162/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0450 - acc: 0.9855\n",
      "Epoch 00162: val_loss did not improve from 0.06342\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0450 - acc: 0.9855 - val_loss: 0.0825 - val_acc: 0.9757\n",
      "Epoch 163/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0380 - acc: 0.9878\n",
      "Epoch 00163: val_loss did not improve from 0.06342\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0380 - acc: 0.9878 - val_loss: 0.1015 - val_acc: 0.9714\n",
      "Epoch 164/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0372 - acc: 0.9881\n",
      "Epoch 00164: val_loss did not improve from 0.06342\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0372 - acc: 0.9881 - val_loss: 0.0761 - val_acc: 0.9767\n",
      "Epoch 165/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0441 - acc: 0.9862\n",
      "Epoch 00165: val_loss did not improve from 0.06342\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0441 - acc: 0.9862 - val_loss: 0.0931 - val_acc: 0.9737\n",
      "Epoch 166/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0386 - acc: 0.9881\n",
      "Epoch 00166: val_loss did not improve from 0.06342\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0386 - acc: 0.9881 - val_loss: 0.0687 - val_acc: 0.9807\n",
      "Epoch 167/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0384 - acc: 0.9879\n",
      "Epoch 00167: val_loss did not improve from 0.06342\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0384 - acc: 0.9879 - val_loss: 0.0703 - val_acc: 0.9797\n",
      "Epoch 168/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0359 - acc: 0.9893\n",
      "Epoch 00168: val_loss did not improve from 0.06342\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0359 - acc: 0.9893 - val_loss: 0.0773 - val_acc: 0.9794\n",
      "Epoch 169/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0405 - acc: 0.9871\n",
      "Epoch 00169: val_loss did not improve from 0.06342\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0405 - acc: 0.9871 - val_loss: 0.0872 - val_acc: 0.9754\n",
      "Epoch 170/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0325 - acc: 0.9900\n",
      "Epoch 00170: val_loss did not improve from 0.06342\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0325 - acc: 0.9900 - val_loss: 0.0886 - val_acc: 0.9750\n",
      "Epoch 171/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0534 - acc: 0.9824\n",
      "Epoch 00171: val_loss did not improve from 0.06342\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0534 - acc: 0.9824 - val_loss: 0.0887 - val_acc: 0.9764\n",
      "Epoch 172/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0447 - acc: 0.9853\n",
      "Epoch 00172: val_loss did not improve from 0.06342\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0447 - acc: 0.9853 - val_loss: 0.0964 - val_acc: 0.9740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 173/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0387 - acc: 0.9881\n",
      "Epoch 00173: val_loss did not improve from 0.06342\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0387 - acc: 0.9881 - val_loss: 0.0973 - val_acc: 0.9737\n",
      "Epoch 174/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0405 - acc: 0.9863\n",
      "Epoch 00174: val_loss did not improve from 0.06342\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0405 - acc: 0.9863 - val_loss: 0.0831 - val_acc: 0.9780\n",
      "Epoch 175/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0379 - acc: 0.9882\n",
      "Epoch 00175: val_loss did not improve from 0.06342\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0379 - acc: 0.9882 - val_loss: 0.0836 - val_acc: 0.9780\n",
      "Epoch 176/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0360 - acc: 0.9880\n",
      "Epoch 00176: val_loss did not improve from 0.06342\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0360 - acc: 0.9880 - val_loss: 0.0741 - val_acc: 0.9797\n",
      "Epoch 177/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0393 - acc: 0.9875\n",
      "Epoch 00177: val_loss did not improve from 0.06342\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0393 - acc: 0.9875 - val_loss: 0.0933 - val_acc: 0.9784\n",
      "Epoch 178/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0402 - acc: 0.9869\n",
      "Epoch 00178: val_loss did not improve from 0.06342\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0402 - acc: 0.9869 - val_loss: 0.1014 - val_acc: 0.9717\n",
      "Epoch 179/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0456 - acc: 0.9856\n",
      "Epoch 00179: val_loss did not improve from 0.06342\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0456 - acc: 0.9856 - val_loss: 0.0887 - val_acc: 0.9737\n",
      "Epoch 180/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0369 - acc: 0.9884\n",
      "Epoch 00180: val_loss did not improve from 0.06342\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0369 - acc: 0.9884 - val_loss: 0.0773 - val_acc: 0.9750\n",
      "Epoch 181/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0319 - acc: 0.9899\n",
      "Epoch 00181: val_loss did not improve from 0.06342\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0319 - acc: 0.9899 - val_loss: 0.0657 - val_acc: 0.9787\n",
      "Epoch 182/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0373 - acc: 0.9886\n",
      "Epoch 00182: val_loss did not improve from 0.06342\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0373 - acc: 0.9886 - val_loss: 0.0693 - val_acc: 0.9800\n",
      "Epoch 183/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0319 - acc: 0.9902\n",
      "Epoch 00183: val_loss did not improve from 0.06342\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0319 - acc: 0.9902 - val_loss: 0.0723 - val_acc: 0.9787\n",
      "Epoch 184/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0445 - acc: 0.9854\n",
      "Epoch 00184: val_loss did not improve from 0.06342\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0445 - acc: 0.9854 - val_loss: 0.0758 - val_acc: 0.9754\n",
      "Epoch 185/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0379 - acc: 0.9876\n",
      "Epoch 00185: val_loss improved from 0.06342 to 0.05915, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.0379 - acc: 0.9876 - val_loss: 0.0592 - val_acc: 0.9834\n",
      "Epoch 186/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0312 - acc: 0.9903\n",
      "Epoch 00186: val_loss did not improve from 0.05915\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0312 - acc: 0.9903 - val_loss: 0.0742 - val_acc: 0.9800\n",
      "Epoch 187/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0351 - acc: 0.9886\n",
      "Epoch 00187: val_loss did not improve from 0.05915\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0351 - acc: 0.9886 - val_loss: 0.0701 - val_acc: 0.9780\n",
      "Epoch 188/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0618 - acc: 0.9800\n",
      "Epoch 00188: val_loss did not improve from 0.05915\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0618 - acc: 0.9800 - val_loss: 0.1016 - val_acc: 0.9687\n",
      "Epoch 189/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0419 - acc: 0.9860\n",
      "Epoch 00189: val_loss did not improve from 0.05915\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0419 - acc: 0.9860 - val_loss: 0.0781 - val_acc: 0.9750\n",
      "Epoch 190/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0286 - acc: 0.9910\n",
      "Epoch 00190: val_loss did not improve from 0.05915\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0286 - acc: 0.9910 - val_loss: 0.0733 - val_acc: 0.9777\n",
      "Epoch 191/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0385 - acc: 0.9876\n",
      "Epoch 00191: val_loss did not improve from 0.05915\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0385 - acc: 0.9876 - val_loss: 0.0838 - val_acc: 0.9760\n",
      "Epoch 192/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0282 - acc: 0.9915\n",
      "Epoch 00192: val_loss did not improve from 0.05915\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0282 - acc: 0.9915 - val_loss: 0.0774 - val_acc: 0.9770\n",
      "Epoch 193/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0353 - acc: 0.9888\n",
      "Epoch 00193: val_loss did not improve from 0.05915\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0353 - acc: 0.9888 - val_loss: 0.0751 - val_acc: 0.9767\n",
      "Epoch 194/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0268 - acc: 0.9918\n",
      "Epoch 00194: val_loss did not improve from 0.05915\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0268 - acc: 0.9918 - val_loss: 0.0790 - val_acc: 0.9784\n",
      "Epoch 195/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0349 - acc: 0.9889\n",
      "Epoch 00195: val_loss did not improve from 0.05915\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0349 - acc: 0.9889 - val_loss: 0.0683 - val_acc: 0.9787\n",
      "Epoch 196/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0510 - acc: 0.9839\n",
      "Epoch 00196: val_loss did not improve from 0.05915\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0510 - acc: 0.9839 - val_loss: 0.0690 - val_acc: 0.9767\n",
      "Epoch 197/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0301 - acc: 0.9903\n",
      "Epoch 00197: val_loss did not improve from 0.05915\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0301 - acc: 0.9903 - val_loss: 0.0822 - val_acc: 0.9787\n",
      "Epoch 198/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0340 - acc: 0.9899\n",
      "Epoch 00198: val_loss did not improve from 0.05915\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0340 - acc: 0.9899 - val_loss: 0.0755 - val_acc: 0.9767\n",
      "Epoch 199/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0271 - acc: 0.9920\n",
      "Epoch 00199: val_loss did not improve from 0.05915\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0271 - acc: 0.9920 - val_loss: 0.0699 - val_acc: 0.9797\n",
      "Epoch 200/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0373 - acc: 0.9881\n",
      "Epoch 00200: val_loss did not improve from 0.05915\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0373 - acc: 0.9881 - val_loss: 0.0685 - val_acc: 0.9794\n",
      "Epoch 201/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0312 - acc: 0.9900\n",
      "Epoch 00201: val_loss improved from 0.05915 to 0.05850, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.0312 - acc: 0.9900 - val_loss: 0.0585 - val_acc: 0.9820\n",
      "Epoch 202/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0362 - acc: 0.9886\n",
      "Epoch 00202: val_loss did not improve from 0.05850\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0362 - acc: 0.9886 - val_loss: 0.0615 - val_acc: 0.9814\n",
      "Epoch 203/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - ETA: 0s - loss: 0.0299 - acc: 0.9897\n",
      "Epoch 00203: val_loss improved from 0.05850 to 0.05846, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.0299 - acc: 0.9897 - val_loss: 0.0585 - val_acc: 0.9857\n",
      "Epoch 204/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0276 - acc: 0.9919\n",
      "Epoch 00204: val_loss did not improve from 0.05846\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0276 - acc: 0.9919 - val_loss: 0.0727 - val_acc: 0.9774\n",
      "Epoch 205/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0293 - acc: 0.9907\n",
      "Epoch 00205: val_loss did not improve from 0.05846\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0293 - acc: 0.9907 - val_loss: 0.0587 - val_acc: 0.9827\n",
      "Epoch 206/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0504 - acc: 0.9844\n",
      "Epoch 00206: val_loss did not improve from 0.05846\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0504 - acc: 0.9844 - val_loss: 0.0642 - val_acc: 0.9814\n",
      "Epoch 207/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0393 - acc: 0.9873\n",
      "Epoch 00207: val_loss did not improve from 0.05846\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0393 - acc: 0.9873 - val_loss: 0.0592 - val_acc: 0.9807\n",
      "Epoch 208/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0339 - acc: 0.9894\n",
      "Epoch 00208: val_loss did not improve from 0.05846\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0339 - acc: 0.9894 - val_loss: 0.0598 - val_acc: 0.9807\n",
      "Epoch 209/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0447 - acc: 0.9859\n",
      "Epoch 00209: val_loss did not improve from 0.05846\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0447 - acc: 0.9859 - val_loss: 0.0666 - val_acc: 0.9794\n",
      "Epoch 210/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0322 - acc: 0.9895\n",
      "Epoch 00210: val_loss improved from 0.05846 to 0.05628, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.0322 - acc: 0.9895 - val_loss: 0.0563 - val_acc: 0.9824\n",
      "Epoch 211/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0350 - acc: 0.9886\n",
      "Epoch 00211: val_loss did not improve from 0.05628\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0350 - acc: 0.9886 - val_loss: 0.0570 - val_acc: 0.9840\n",
      "Epoch 212/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0295 - acc: 0.9905\n",
      "Epoch 00212: val_loss did not improve from 0.05628\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0295 - acc: 0.9905 - val_loss: 0.0574 - val_acc: 0.9804\n",
      "Epoch 213/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0268 - acc: 0.9919\n",
      "Epoch 00213: val_loss did not improve from 0.05628\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0268 - acc: 0.9919 - val_loss: 0.0571 - val_acc: 0.9824\n",
      "Epoch 214/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0256 - acc: 0.9920\n",
      "Epoch 00214: val_loss improved from 0.05628 to 0.05131, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.0256 - acc: 0.9920 - val_loss: 0.0513 - val_acc: 0.9850\n",
      "Epoch 215/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0274 - acc: 0.9918\n",
      "Epoch 00215: val_loss did not improve from 0.05131\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0274 - acc: 0.9918 - val_loss: 0.0524 - val_acc: 0.9850\n",
      "Epoch 216/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0286 - acc: 0.9914\n",
      "Epoch 00216: val_loss improved from 0.05131 to 0.04871, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.0286 - acc: 0.9914 - val_loss: 0.0487 - val_acc: 0.9847\n",
      "Epoch 217/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0280 - acc: 0.9910\n",
      "Epoch 00217: val_loss did not improve from 0.04871\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0280 - acc: 0.9910 - val_loss: 0.0539 - val_acc: 0.9850\n",
      "Epoch 218/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0275 - acc: 0.9911\n",
      "Epoch 00218: val_loss did not improve from 0.04871\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0275 - acc: 0.9911 - val_loss: 0.0611 - val_acc: 0.9794\n",
      "Epoch 219/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0350 - acc: 0.9889\n",
      "Epoch 00219: val_loss did not improve from 0.04871\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0350 - acc: 0.9889 - val_loss: 0.0557 - val_acc: 0.9847\n",
      "Epoch 220/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0398 - acc: 0.9880\n",
      "Epoch 00220: val_loss did not improve from 0.04871\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0398 - acc: 0.9880 - val_loss: 0.0560 - val_acc: 0.9807\n",
      "Epoch 221/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0341 - acc: 0.9890\n",
      "Epoch 00221: val_loss did not improve from 0.04871\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0341 - acc: 0.9890 - val_loss: 0.0656 - val_acc: 0.9830\n",
      "Epoch 222/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0271 - acc: 0.9916\n",
      "Epoch 00222: val_loss did not improve from 0.04871\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0271 - acc: 0.9916 - val_loss: 0.0510 - val_acc: 0.9837\n",
      "Epoch 223/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0295 - acc: 0.9912\n",
      "Epoch 00223: val_loss did not improve from 0.04871\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0295 - acc: 0.9912 - val_loss: 0.0560 - val_acc: 0.9814\n",
      "Epoch 224/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0316 - acc: 0.9906\n",
      "Epoch 00224: val_loss did not improve from 0.04871\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0316 - acc: 0.9906 - val_loss: 0.0568 - val_acc: 0.9814\n",
      "Epoch 225/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0245 - acc: 0.9926\n",
      "Epoch 00225: val_loss did not improve from 0.04871\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0245 - acc: 0.9926 - val_loss: 0.0690 - val_acc: 0.9784\n",
      "Epoch 226/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0413 - acc: 0.9869\n",
      "Epoch 00226: val_loss did not improve from 0.04871\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0413 - acc: 0.9869 - val_loss: 0.0654 - val_acc: 0.9797\n",
      "Epoch 227/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0279 - acc: 0.9906\n",
      "Epoch 00227: val_loss did not improve from 0.04871\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0279 - acc: 0.9906 - val_loss: 0.0519 - val_acc: 0.9850\n",
      "Epoch 228/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0324 - acc: 0.9901\n",
      "Epoch 00228: val_loss did not improve from 0.04871\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0324 - acc: 0.9901 - val_loss: 0.0564 - val_acc: 0.9830\n",
      "Epoch 229/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0263 - acc: 0.9918\n",
      "Epoch 00229: val_loss did not improve from 0.04871\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0263 - acc: 0.9918 - val_loss: 0.0684 - val_acc: 0.9794\n",
      "Epoch 230/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0243 - acc: 0.9925\n",
      "Epoch 00230: val_loss did not improve from 0.04871\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0243 - acc: 0.9925 - val_loss: 0.0540 - val_acc: 0.9827\n",
      "Epoch 231/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0248 - acc: 0.9926\n",
      "Epoch 00231: val_loss did not improve from 0.04871\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0248 - acc: 0.9926 - val_loss: 0.0556 - val_acc: 0.9850\n",
      "Epoch 232/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0247 - acc: 0.9927\n",
      "Epoch 00232: val_loss did not improve from 0.04871\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0247 - acc: 0.9927 - val_loss: 0.0605 - val_acc: 0.9827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 233/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0259 - acc: 0.9921\n",
      "Epoch 00233: val_loss did not improve from 0.04871\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0259 - acc: 0.9921 - val_loss: 0.0487 - val_acc: 0.9837\n",
      "Epoch 234/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0251 - acc: 0.9922\n",
      "Epoch 00234: val_loss did not improve from 0.04871\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0251 - acc: 0.9922 - val_loss: 0.0559 - val_acc: 0.9840\n",
      "Epoch 235/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0227 - acc: 0.9929\n",
      "Epoch 00235: val_loss improved from 0.04871 to 0.04806, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.0227 - acc: 0.9929 - val_loss: 0.0481 - val_acc: 0.9854\n",
      "Epoch 236/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0237 - acc: 0.9924\n",
      "Epoch 00236: val_loss did not improve from 0.04806\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0237 - acc: 0.9924 - val_loss: 0.0499 - val_acc: 0.9864\n",
      "Epoch 237/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0311 - acc: 0.9904\n",
      "Epoch 00237: val_loss did not improve from 0.04806\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0311 - acc: 0.9904 - val_loss: 0.0671 - val_acc: 0.9794\n",
      "Epoch 238/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0278 - acc: 0.9917\n",
      "Epoch 00238: val_loss did not improve from 0.04806\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0278 - acc: 0.9917 - val_loss: 0.0707 - val_acc: 0.9804\n",
      "Epoch 239/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0265 - acc: 0.9914\n",
      "Epoch 00239: val_loss did not improve from 0.04806\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0265 - acc: 0.9914 - val_loss: 0.0654 - val_acc: 0.9787\n",
      "Epoch 240/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0300 - acc: 0.9905\n",
      "Epoch 00240: val_loss did not improve from 0.04806\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0300 - acc: 0.9905 - val_loss: 0.0578 - val_acc: 0.9810\n",
      "Epoch 241/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0303 - acc: 0.9906\n",
      "Epoch 00241: val_loss did not improve from 0.04806\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0303 - acc: 0.9906 - val_loss: 0.0816 - val_acc: 0.9770\n",
      "Epoch 242/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0261 - acc: 0.9922\n",
      "Epoch 00242: val_loss did not improve from 0.04806\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0261 - acc: 0.9922 - val_loss: 0.0579 - val_acc: 0.9807\n",
      "Epoch 243/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0214 - acc: 0.9940\n",
      "Epoch 00243: val_loss did not improve from 0.04806\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0214 - acc: 0.9940 - val_loss: 0.0628 - val_acc: 0.9790\n",
      "Epoch 244/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0342 - acc: 0.9893\n",
      "Epoch 00244: val_loss did not improve from 0.04806\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0342 - acc: 0.9893 - val_loss: 0.0617 - val_acc: 0.9797\n",
      "Epoch 245/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0297 - acc: 0.9911\n",
      "Epoch 00245: val_loss did not improve from 0.04806\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0297 - acc: 0.9911 - val_loss: 0.0483 - val_acc: 0.9857\n",
      "Epoch 246/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0220 - acc: 0.9931\n",
      "Epoch 00246: val_loss improved from 0.04806 to 0.04635, saving model to ../model_files/base_cnn_8_seconds_ppg_acl_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.0220 - acc: 0.9931 - val_loss: 0.0464 - val_acc: 0.9840\n",
      "Epoch 247/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0231 - acc: 0.9930\n",
      "Epoch 00247: val_loss did not improve from 0.04635\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0231 - acc: 0.9930 - val_loss: 0.0546 - val_acc: 0.9820\n",
      "Epoch 248/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0260 - acc: 0.9921\n",
      "Epoch 00248: val_loss did not improve from 0.04635\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0260 - acc: 0.9921 - val_loss: 0.0576 - val_acc: 0.9827\n",
      "Epoch 249/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0381 - acc: 0.9880\n",
      "Epoch 00249: val_loss did not improve from 0.04635\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0381 - acc: 0.9880 - val_loss: 0.0567 - val_acc: 0.9810\n",
      "Epoch 250/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0326 - acc: 0.9895\n",
      "Epoch 00250: val_loss did not improve from 0.04635\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0326 - acc: 0.9895 - val_loss: 0.0622 - val_acc: 0.9817\n",
      "Epoch 251/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0261 - acc: 0.9918\n",
      "Epoch 00251: val_loss did not improve from 0.04635\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0261 - acc: 0.9918 - val_loss: 0.0628 - val_acc: 0.9814\n",
      "Epoch 252/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0346 - acc: 0.9887\n",
      "Epoch 00252: val_loss did not improve from 0.04635\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0346 - acc: 0.9887 - val_loss: 0.0594 - val_acc: 0.9827\n",
      "Epoch 253/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0304 - acc: 0.9904\n",
      "Epoch 00253: val_loss did not improve from 0.04635\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0304 - acc: 0.9904 - val_loss: 0.0492 - val_acc: 0.9844\n",
      "Epoch 254/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0260 - acc: 0.9920\n",
      "Epoch 00254: val_loss did not improve from 0.04635\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0260 - acc: 0.9920 - val_loss: 0.0681 - val_acc: 0.9804\n",
      "Epoch 255/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0654 - acc: 0.9787\n",
      "Epoch 00255: val_loss did not improve from 0.04635\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0654 - acc: 0.9787 - val_loss: 0.0541 - val_acc: 0.9840\n",
      "Epoch 256/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0308 - acc: 0.9902\n",
      "Epoch 00256: val_loss did not improve from 0.04635\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0308 - acc: 0.9902 - val_loss: 0.0637 - val_acc: 0.9800\n",
      "Epoch 257/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0351 - acc: 0.9897\n",
      "Epoch 00257: val_loss did not improve from 0.04635\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0351 - acc: 0.9897 - val_loss: 0.0575 - val_acc: 0.9840\n",
      "Epoch 258/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0221 - acc: 0.9920\n",
      "Epoch 00258: val_loss did not improve from 0.04635\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0221 - acc: 0.9920 - val_loss: 0.0482 - val_acc: 0.9850\n",
      "Epoch 259/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0210 - acc: 0.9934\n",
      "Epoch 00259: val_loss did not improve from 0.04635\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0210 - acc: 0.9934 - val_loss: 0.0521 - val_acc: 0.9850\n",
      "Epoch 260/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0195 - acc: 0.9943\n",
      "Epoch 00260: val_loss did not improve from 0.04635\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0195 - acc: 0.9943 - val_loss: 0.0520 - val_acc: 0.9850\n",
      "Epoch 261/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0225 - acc: 0.9929\n",
      "Epoch 00261: val_loss did not improve from 0.04635\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0225 - acc: 0.9929 - val_loss: 0.0734 - val_acc: 0.9774\n",
      "Epoch 262/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0274 - acc: 0.9909\n",
      "Epoch 00262: val_loss did not improve from 0.04635\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0274 - acc: 0.9909 - val_loss: 0.0627 - val_acc: 0.9814\n",
      "Epoch 263/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - ETA: 0s - loss: 0.0277 - acc: 0.9907\n",
      "Epoch 00263: val_loss did not improve from 0.04635\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0277 - acc: 0.9907 - val_loss: 0.0576 - val_acc: 0.9827\n",
      "Epoch 264/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0223 - acc: 0.9929\n",
      "Epoch 00264: val_loss did not improve from 0.04635\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0223 - acc: 0.9929 - val_loss: 0.0549 - val_acc: 0.9844\n",
      "Epoch 265/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0260 - acc: 0.9917\n",
      "Epoch 00265: val_loss did not improve from 0.04635\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0260 - acc: 0.9917 - val_loss: 0.0500 - val_acc: 0.9840\n",
      "Epoch 266/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0271 - acc: 0.9916\n",
      "Epoch 00266: val_loss did not improve from 0.04635\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0271 - acc: 0.9916 - val_loss: 0.0557 - val_acc: 0.9850\n",
      "Epoch 267/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0234 - acc: 0.9922\n",
      "Epoch 00267: val_loss did not improve from 0.04635\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0234 - acc: 0.9922 - val_loss: 0.0656 - val_acc: 0.9797\n",
      "Epoch 268/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0275 - acc: 0.9910\n",
      "Epoch 00268: val_loss did not improve from 0.04635\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0275 - acc: 0.9910 - val_loss: 0.0493 - val_acc: 0.9834\n",
      "Epoch 269/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0339 - acc: 0.9893\n",
      "Epoch 00269: val_loss did not improve from 0.04635\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0339 - acc: 0.9893 - val_loss: 0.0481 - val_acc: 0.9830\n",
      "Epoch 270/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0225 - acc: 0.9927\n",
      "Epoch 00270: val_loss did not improve from 0.04635\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0225 - acc: 0.9927 - val_loss: 0.0515 - val_acc: 0.9840\n",
      "Epoch 271/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0239 - acc: 0.9925\n",
      "Epoch 00271: val_loss did not improve from 0.04635\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0239 - acc: 0.9925 - val_loss: 0.0710 - val_acc: 0.9770\n",
      "Epoch 272/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0296 - acc: 0.9903\n",
      "Epoch 00272: val_loss did not improve from 0.04635\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0296 - acc: 0.9903 - val_loss: 0.0677 - val_acc: 0.9784\n",
      "Epoch 273/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0276 - acc: 0.9913\n",
      "Epoch 00273: val_loss did not improve from 0.04635\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0276 - acc: 0.9913 - val_loss: 0.0489 - val_acc: 0.9860\n",
      "Epoch 274/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0366 - acc: 0.9879\n",
      "Epoch 00274: val_loss did not improve from 0.04635\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0366 - acc: 0.9879 - val_loss: 0.0522 - val_acc: 0.9817\n",
      "Epoch 275/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0235 - acc: 0.9923\n",
      "Epoch 00275: val_loss did not improve from 0.04635\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0235 - acc: 0.9923 - val_loss: 0.0602 - val_acc: 0.9790\n",
      "Epoch 276/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0245 - acc: 0.9923\n",
      "Epoch 00276: val_loss did not improve from 0.04635\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0245 - acc: 0.9923 - val_loss: 0.0645 - val_acc: 0.9817\n",
      "Epoch 277/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0278 - acc: 0.9916\n",
      "Epoch 00277: val_loss did not improve from 0.04635\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0278 - acc: 0.9916 - val_loss: 0.0591 - val_acc: 0.9824\n",
      "Epoch 278/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0231 - acc: 0.9927\n",
      "Epoch 00278: val_loss did not improve from 0.04635\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0231 - acc: 0.9927 - val_loss: 0.0573 - val_acc: 0.9850\n",
      "Epoch 279/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0254 - acc: 0.9919\n",
      "Epoch 00279: val_loss did not improve from 0.04635\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0254 - acc: 0.9919 - val_loss: 0.0590 - val_acc: 0.9814\n",
      "Epoch 280/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0236 - acc: 0.9933\n",
      "Epoch 00280: val_loss did not improve from 0.04635\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0236 - acc: 0.9933 - val_loss: 0.0484 - val_acc: 0.9864\n",
      "Epoch 281/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0278 - acc: 0.9916\n",
      "Epoch 00281: val_loss did not improve from 0.04635\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0278 - acc: 0.9916 - val_loss: 0.0628 - val_acc: 0.9804\n",
      "Epoch 282/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0240 - acc: 0.9925\n",
      "Epoch 00282: val_loss did not improve from 0.04635\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0240 - acc: 0.9925 - val_loss: 0.0586 - val_acc: 0.9824\n",
      "Epoch 283/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0197 - acc: 0.9939\n",
      "Epoch 00283: val_loss did not improve from 0.04635\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0197 - acc: 0.9939 - val_loss: 0.0486 - val_acc: 0.9860\n",
      "Epoch 284/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0280 - acc: 0.9913\n",
      "Epoch 00284: val_loss did not improve from 0.04635\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0280 - acc: 0.9913 - val_loss: 0.0538 - val_acc: 0.9854\n",
      "Epoch 285/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0279 - acc: 0.9911\n",
      "Epoch 00285: val_loss did not improve from 0.04635\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0279 - acc: 0.9911 - val_loss: 0.0589 - val_acc: 0.9837\n",
      "Epoch 286/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0228 - acc: 0.9935\n",
      "Epoch 00286: val_loss did not improve from 0.04635\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.0228 - acc: 0.9935 - val_loss: 0.0518 - val_acc: 0.9847\n",
      "Epoch 00286: early stopping\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=40)\n",
    "callbacks_list = [es,checkpoint]\n",
    "history = model.fit(train_x,train_participant,validation_data=(val_x,val_participant), epochs=400, batch_size=200,\n",
    "          callbacks=callbacks_list,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(filepath)\n",
    "test_participant_pred = model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(np.argmax(test_participant,axis=1),np.argmax(test_participant_pred,axis=1)))\n",
    "print(accuracy_score(np.argmax(test_participant,axis=1),np.argmax(test_participant_pred,axis=1)))\n",
    "#filtered_acl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(np.argmax(test_participant,axis=1),np.argmax(test_participant_pred,axis=1)))\n",
    "print(accuracy_score(np.argmax(test_participant,axis=1),np.argmax(test_participant_pred,axis=1)))\n",
    "#filtered ppg+filtered_acl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(np.argmax(test_participant,axis=1),np.argmax(test_participant_pred,axis=1)))\n",
    "print(accuracy_score(np.argmax(test_participant,axis=1),np.argmax(test_participant_pred,axis=1)))\n",
    "#filtered ppg+acl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(np.argmax(test_participant,axis=1),np.argmax(test_participant_pred,axis=1)))\n",
    "print(accuracy_score(np.argmax(test_participant,axis=1),np.argmax(test_participant_pred,axis=1)))\n",
    "#filtered ppg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[517   0   2   0   0   0   0   3   0   2   0   0   0]\n",
      " [  0 647   0   0   1   0   0   0   0   0   0   0   0]\n",
      " [  4   0 508   1   0   0   0   3   6   0   0   1   0]\n",
      " [  1   0   0 701   0   0   0   0   1   3   0   0   0]\n",
      " [  0  13   0   0 576   0   0   0   0   0   0   0   2]\n",
      " [  0   0   0   0   0 548   1   0   3   1   0   0   0]\n",
      " [  0   0   0   0   0   0 540   1   8   0   0   0   0]\n",
      " [  1   0   1   0   0   0   3 613   0   4   0   3   0]\n",
      " [  2   0   1   1   0   0   0   0 556   0   0   0   2]\n",
      " [  4   0   2   2   0   0   3   2   1 502   2   3   0]\n",
      " [  0   0   0   1   0   3   1   2   1   3 531   3   1]\n",
      " [  1   0   0   2   0   2   0   4   3   1   1 593   0]\n",
      " [  0   2   0   0   0   0   0   0   0   0   0   0 552]]\n",
      "0.9833533093620989\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(np.argmax(test_participant,axis=1),np.argmax(test_participant_pred,axis=1)))\n",
    "print(accuracy_score(np.argmax(test_participant,axis=1),np.argmax(test_participant_pred,axis=1)))\n",
    "#ppg+acl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(np.argmax(test_participant,axis=1),np.argmax(test_participant_pred,axis=1)))\n",
    "print(accuracy_score(np.argmax(test_participant,axis=1),np.argmax(test_participant_pred,axis=1)))\n",
    "#ppg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(np.argmax(test_participant,axis=1),np.argmax(test_participant_pred,axis=1)))\n",
    "print(accuracy_score(np.argmax(test_participant,axis=1),np.argmax(test_participant_pred,axis=1)))\n",
    "#acl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
