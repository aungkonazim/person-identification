{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "X_acl,X_ppg,y,y_participant,X_time = pickle.load(open('../data/tabular_data.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_ppg = X_ppg[:,np.arange(0,512,2),:]\n",
    "X_all = np.concatenate([X_ppg,X_acl],axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43363, 256, 8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37544, 256, 3), (37544,), (5819,), (5819, 256, 3))"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logo = LeaveOneGroupOut()\n",
    "logo = LeavePGroupsOut(n_groups=2)\n",
    "for train_index, test_index in logo.split(X_all, y, y_participant.reshape(-1)):\n",
    "    train_x, test_x_unseen = X_all[train_index], X_all[test_index]\n",
    "    train_y, test_y_unseen = y[train_index], y[test_index]\n",
    "    train_participant, test_participant_unseen = y_participant[train_index], y_participant[test_index]\n",
    "    train_participant, test_participant_unseen = X_time[train_index], X_time[test_index]\n",
    "    break\n",
    "train_x.shape,train_y.shape,test_y_unseen.shape,test_x_unseen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelBinarizer,LabelEncoder\n",
    "\n",
    "participant_ids = train_participant.copy()\n",
    "train_participant = OneHotEncoder().fit_transform(train_participant.reshape(-1,1)).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y,train_participant, test_participant,participant_ids_train, participant_ids_test = train_test_split(train_x,\n",
    "                                                            train_y,\n",
    "                                                            train_participant,\n",
    "                                                            participant_ids,\n",
    "                                                            test_size = 0.2,\n",
    "                                                            random_state=42,\n",
    "                                                            stratify=participant_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, val_x, train_y, val_y, train_participant,val_participant,participant_ids_train, participant_ids_val  = train_test_split(train_x,\n",
    "                                                            train_y,\n",
    "                                                            train_participant,\n",
    "                                                            participant_ids_train,\n",
    "                                                            test_size = 0.1,\n",
    "                                                            random_state=42,\n",
    "                                                            stratify=participant_ids_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((27031, 256, 3),\n",
       " (7509, 256, 3),\n",
       " (3004, 256, 3),\n",
       " (27031, 13),\n",
       " (7509, 13),\n",
       " (3004, 13))"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape,test_x.shape,val_x.shape,train_participant.shape,test_participant.shape,val_participant.shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_self_attention import SeqSelfAttention\n",
    "from keras import backend as K\n",
    "from keras.layers import Concatenate, Flatten,LeakyReLU, Activation, RepeatVector, Permute, Multiply, Lambda, Dense, merge# Define a regular layer instead of writing a custom layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_shape=(256,1),act='relu',loss=\"categorical_crossentropy\",opt='adam',n_classes=350):\n",
    "    model =  Sequential()\n",
    "    model.add(Conv1D(100,10,input_shape=input_shape,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(.2))\n",
    "    model.add(Conv1D(100,10,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(.2))\n",
    "    model.add(Conv1D(100,10,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(50,2,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(n_classes,activation='sigmoid',kernel_initializer='normal'))\n",
    "    model.add(Dense(n_classes,activation='softmax',kernel_initializer='normal'))\n",
    "    model.compile(loss=loss,optimizer=opt,metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_48 (Conv1D)           (None, 256, 100)          3100      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 128, 100)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_48 (Batc (None, 128, 100)          400       \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 128, 100)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 128, 100)          100100    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 64, 100)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_49 (Batc (None, 64, 100)           400       \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 64, 100)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 64, 100)           100100    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 32, 100)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_50 (Batc (None, 32, 100)           400       \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 32, 50)            10050     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 16, 50)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_51 (Batc (None, 16, 50)            200       \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 16, 50)            0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 13)                10413     \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 13)                182       \n",
      "=================================================================\n",
      "Total params: 225,345\n",
      "Trainable params: 224,645\n",
      "Non-trainable params: 700\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_classes = len(np.unique(participant_ids_train))\n",
    "model = get_model(input_shape=(256,3),n_classes=n_classes)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "filepath = '../model_files/base_cnn_8_seconds_acl_filtered_wesad.hdf5'\n",
    "# model = load_model(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.9999 - acc: 0.6443\n",
      "Epoch 00001: val_loss improved from inf to 2.01921, saving model to ../model_files/base_cnn_8_seconds_acl_filtered_wesad.hdf5\n",
      "136/136 [==============================] - 2s 14ms/step - loss: 0.9999 - acc: 0.6443 - val_loss: 2.0192 - val_acc: 0.4314\n",
      "Epoch 2/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.9830 - acc: 0.6503\n",
      "Epoch 00002: val_loss did not improve from 2.01921\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.9830 - acc: 0.6503 - val_loss: 6.7162 - val_acc: 0.2563\n",
      "Epoch 3/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.9771 - acc: 0.6499\n",
      "Epoch 00003: val_loss did not improve from 2.01921\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.9771 - acc: 0.6499 - val_loss: 4.0208 - val_acc: 0.3479\n",
      "Epoch 4/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.9635 - acc: 0.6557\n",
      "Epoch 00004: val_loss improved from 2.01921 to 2.01317, saving model to ../model_files/base_cnn_8_seconds_acl_filtered_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.9635 - acc: 0.6557 - val_loss: 2.0132 - val_acc: 0.4274\n",
      "Epoch 5/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.9641 - acc: 0.6609\n",
      "Epoch 00005: val_loss did not improve from 2.01317\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.9641 - acc: 0.6609 - val_loss: 5.6093 - val_acc: 0.2843\n",
      "Epoch 6/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.9602 - acc: 0.6609\n",
      "Epoch 00006: val_loss did not improve from 2.01317\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.9602 - acc: 0.6609 - val_loss: 2.1645 - val_acc: 0.3908\n",
      "Epoch 7/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.9655 - acc: 0.6567\n",
      "Epoch 00007: val_loss did not improve from 2.01317\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.9655 - acc: 0.6567 - val_loss: 3.1066 - val_acc: 0.3958\n",
      "Epoch 8/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.9493 - acc: 0.6615\n",
      "Epoch 00008: val_loss did not improve from 2.01317\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.9493 - acc: 0.6615 - val_loss: 3.8125 - val_acc: 0.2913\n",
      "Epoch 9/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.9421 - acc: 0.6666\n",
      "Epoch 00009: val_loss did not improve from 2.01317\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.9421 - acc: 0.6666 - val_loss: 6.0256 - val_acc: 0.2969\n",
      "Epoch 10/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.9350 - acc: 0.6686\n",
      "Epoch 00010: val_loss did not improve from 2.01317\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.9350 - acc: 0.6686 - val_loss: 3.6427 - val_acc: 0.3166\n",
      "Epoch 11/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.9300 - acc: 0.6703\n",
      "Epoch 00011: val_loss did not improve from 2.01317\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.9300 - acc: 0.6703 - val_loss: 3.3351 - val_acc: 0.3449\n",
      "Epoch 12/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.9254 - acc: 0.6692\n",
      "Epoch 00012: val_loss improved from 2.01317 to 1.98246, saving model to ../model_files/base_cnn_8_seconds_acl_filtered_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.9254 - acc: 0.6692 - val_loss: 1.9825 - val_acc: 0.4690\n",
      "Epoch 13/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.9190 - acc: 0.6710\n",
      "Epoch 00013: val_loss did not improve from 1.98246\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.9190 - acc: 0.6710 - val_loss: 4.1896 - val_acc: 0.3039\n",
      "Epoch 14/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.9138 - acc: 0.6741\n",
      "Epoch 00014: val_loss did not improve from 1.98246\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.9138 - acc: 0.6741 - val_loss: 3.8814 - val_acc: 0.3582\n",
      "Epoch 15/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.9072 - acc: 0.6811\n",
      "Epoch 00015: val_loss improved from 1.98246 to 1.63640, saving model to ../model_files/base_cnn_8_seconds_acl_filtered_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.9072 - acc: 0.6811 - val_loss: 1.6364 - val_acc: 0.4887\n",
      "Epoch 16/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.8990 - acc: 0.6776\n",
      "Epoch 00016: val_loss did not improve from 1.63640\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.8990 - acc: 0.6776 - val_loss: 2.7910 - val_acc: 0.3798\n",
      "Epoch 17/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.9019 - acc: 0.6797\n",
      "Epoch 00017: val_loss did not improve from 1.63640\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.9019 - acc: 0.6797 - val_loss: 2.5831 - val_acc: 0.3708\n",
      "Epoch 18/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.8974 - acc: 0.6777\n",
      "Epoch 00018: val_loss did not improve from 1.63640\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.8974 - acc: 0.6777 - val_loss: 3.3997 - val_acc: 0.3888\n",
      "Epoch 19/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.8810 - acc: 0.6861\n",
      "Epoch 00019: val_loss did not improve from 1.63640\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.8810 - acc: 0.6861 - val_loss: 4.8191 - val_acc: 0.3043\n",
      "Epoch 20/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.8752 - acc: 0.6924\n",
      "Epoch 00020: val_loss did not improve from 1.63640\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.8752 - acc: 0.6924 - val_loss: 3.0835 - val_acc: 0.3532\n",
      "Epoch 21/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.8588 - acc: 0.6949\n",
      "Epoch 00021: val_loss did not improve from 1.63640\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.8588 - acc: 0.6949 - val_loss: 7.2697 - val_acc: 0.2643\n",
      "Epoch 22/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.8641 - acc: 0.6970\n",
      "Epoch 00022: val_loss did not improve from 1.63640\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.8641 - acc: 0.6970 - val_loss: 3.2598 - val_acc: 0.3838\n",
      "Epoch 23/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.8667 - acc: 0.6937\n",
      "Epoch 00023: val_loss did not improve from 1.63640\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.8667 - acc: 0.6937 - val_loss: 3.4601 - val_acc: 0.3625\n",
      "Epoch 24/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.8615 - acc: 0.6941\n",
      "Epoch 00024: val_loss did not improve from 1.63640\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.8615 - acc: 0.6941 - val_loss: 4.4407 - val_acc: 0.3132\n",
      "Epoch 25/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.8656 - acc: 0.6932\n",
      "Epoch 00025: val_loss did not improve from 1.63640\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.8656 - acc: 0.6932 - val_loss: 6.0029 - val_acc: 0.3202\n",
      "Epoch 26/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.8422 - acc: 0.7026\n",
      "Epoch 00026: val_loss did not improve from 1.63640\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.8422 - acc: 0.7026 - val_loss: 3.8057 - val_acc: 0.3152\n",
      "Epoch 27/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.8469 - acc: 0.6988\n",
      "Epoch 00027: val_loss did not improve from 1.63640\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.8469 - acc: 0.6988 - val_loss: 4.0530 - val_acc: 0.3815\n",
      "Epoch 28/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.8394 - acc: 0.7050\n",
      "Epoch 00028: val_loss did not improve from 1.63640\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.8394 - acc: 0.7050 - val_loss: 6.7490 - val_acc: 0.2956\n",
      "Epoch 29/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.8309 - acc: 0.7072\n",
      "Epoch 00029: val_loss did not improve from 1.63640\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.8309 - acc: 0.7072 - val_loss: 6.4491 - val_acc: 0.3109\n",
      "Epoch 30/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.8300 - acc: 0.7079\n",
      "Epoch 00030: val_loss did not improve from 1.63640\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.8300 - acc: 0.7079 - val_loss: 6.2523 - val_acc: 0.3239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.8376 - acc: 0.7027\n",
      "Epoch 00031: val_loss did not improve from 1.63640\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.8376 - acc: 0.7027 - val_loss: 2.4825 - val_acc: 0.4451\n",
      "Epoch 32/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.8242 - acc: 0.7072\n",
      "Epoch 00032: val_loss did not improve from 1.63640\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.8242 - acc: 0.7072 - val_loss: 4.5028 - val_acc: 0.3462\n",
      "Epoch 33/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.8124 - acc: 0.7158\n",
      "Epoch 00033: val_loss did not improve from 1.63640\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.8124 - acc: 0.7158 - val_loss: 3.3485 - val_acc: 0.3802\n",
      "Epoch 34/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.8160 - acc: 0.7095\n",
      "Epoch 00034: val_loss did not improve from 1.63640\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.8160 - acc: 0.7095 - val_loss: 3.1978 - val_acc: 0.4268\n",
      "Epoch 35/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.8188 - acc: 0.7081\n",
      "Epoch 00035: val_loss did not improve from 1.63640\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.8188 - acc: 0.7081 - val_loss: 3.3657 - val_acc: 0.3728\n",
      "Epoch 36/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.8049 - acc: 0.7165\n",
      "Epoch 00036: val_loss did not improve from 1.63640\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.8049 - acc: 0.7165 - val_loss: 4.1277 - val_acc: 0.3375\n",
      "Epoch 37/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.8049 - acc: 0.7161\n",
      "Epoch 00037: val_loss did not improve from 1.63640\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.8049 - acc: 0.7161 - val_loss: 2.1665 - val_acc: 0.4900\n",
      "Epoch 38/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.7961 - acc: 0.7178\n",
      "Epoch 00038: val_loss did not improve from 1.63640\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.7961 - acc: 0.7178 - val_loss: 3.8819 - val_acc: 0.3299\n",
      "Epoch 39/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.8080 - acc: 0.7154\n",
      "Epoch 00039: val_loss did not improve from 1.63640\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.8080 - acc: 0.7154 - val_loss: 3.2248 - val_acc: 0.3951\n",
      "Epoch 40/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.7918 - acc: 0.7207\n",
      "Epoch 00040: val_loss did not improve from 1.63640\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.7918 - acc: 0.7207 - val_loss: 4.1810 - val_acc: 0.3685\n",
      "Epoch 41/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.8020 - acc: 0.7164\n",
      "Epoch 00041: val_loss did not improve from 1.63640\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.8020 - acc: 0.7164 - val_loss: 6.7429 - val_acc: 0.3179\n",
      "Epoch 42/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.7903 - acc: 0.7226\n",
      "Epoch 00042: val_loss improved from 1.63640 to 1.34566, saving model to ../model_files/base_cnn_8_seconds_acl_filtered_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.7903 - acc: 0.7226 - val_loss: 1.3457 - val_acc: 0.5726\n",
      "Epoch 43/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.7869 - acc: 0.7212\n",
      "Epoch 00043: val_loss did not improve from 1.34566\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.7869 - acc: 0.7212 - val_loss: 3.7540 - val_acc: 0.3818\n",
      "Epoch 44/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.7677 - acc: 0.7291\n",
      "Epoch 00044: val_loss did not improve from 1.34566\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.7677 - acc: 0.7291 - val_loss: 2.9725 - val_acc: 0.4421\n",
      "Epoch 45/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.7659 - acc: 0.7304\n",
      "Epoch 00045: val_loss did not improve from 1.34566\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.7659 - acc: 0.7304 - val_loss: 4.9304 - val_acc: 0.3339\n",
      "Epoch 46/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.7706 - acc: 0.7297\n",
      "Epoch 00046: val_loss did not improve from 1.34566\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.7706 - acc: 0.7297 - val_loss: 2.3378 - val_acc: 0.4487\n",
      "Epoch 47/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.7601 - acc: 0.7285\n",
      "Epoch 00047: val_loss did not improve from 1.34566\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.7601 - acc: 0.7285 - val_loss: 3.2038 - val_acc: 0.3928\n",
      "Epoch 48/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.7482 - acc: 0.7355\n",
      "Epoch 00048: val_loss did not improve from 1.34566\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.7482 - acc: 0.7355 - val_loss: 5.4503 - val_acc: 0.3582\n",
      "Epoch 49/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.7665 - acc: 0.7290\n",
      "Epoch 00049: val_loss did not improve from 1.34566\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.7665 - acc: 0.7290 - val_loss: 5.1758 - val_acc: 0.3216\n",
      "Epoch 50/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.7574 - acc: 0.7338\n",
      "Epoch 00050: val_loss did not improve from 1.34566\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.7574 - acc: 0.7338 - val_loss: 3.8418 - val_acc: 0.3988\n",
      "Epoch 51/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.7621 - acc: 0.7320\n",
      "Epoch 00051: val_loss did not improve from 1.34566\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.7621 - acc: 0.7320 - val_loss: 6.9666 - val_acc: 0.3059\n",
      "Epoch 52/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.7695 - acc: 0.7303\n",
      "Epoch 00052: val_loss did not improve from 1.34566\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.7695 - acc: 0.7303 - val_loss: 3.9533 - val_acc: 0.3482\n",
      "Epoch 53/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.7404 - acc: 0.7377\n",
      "Epoch 00053: val_loss did not improve from 1.34566\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.7404 - acc: 0.7377 - val_loss: 1.5509 - val_acc: 0.5389\n",
      "Epoch 54/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.7375 - acc: 0.7427\n",
      "Epoch 00054: val_loss did not improve from 1.34566\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.7375 - acc: 0.7427 - val_loss: 4.0847 - val_acc: 0.3745\n",
      "Epoch 55/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.7376 - acc: 0.7416\n",
      "Epoch 00055: val_loss did not improve from 1.34566\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.7376 - acc: 0.7416 - val_loss: 2.2241 - val_acc: 0.4657\n",
      "Epoch 56/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.7449 - acc: 0.7376\n",
      "Epoch 00056: val_loss did not improve from 1.34566\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.7449 - acc: 0.7376 - val_loss: 5.8561 - val_acc: 0.3472\n",
      "Epoch 57/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.7340 - acc: 0.7421\n",
      "Epoch 00057: val_loss did not improve from 1.34566\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.7340 - acc: 0.7421 - val_loss: 1.8509 - val_acc: 0.4990\n",
      "Epoch 58/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.7395 - acc: 0.7415\n",
      "Epoch 00058: val_loss did not improve from 1.34566\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.7395 - acc: 0.7415 - val_loss: 6.0610 - val_acc: 0.3292\n",
      "Epoch 59/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.7212 - acc: 0.7444\n",
      "Epoch 00059: val_loss did not improve from 1.34566\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.7212 - acc: 0.7444 - val_loss: 1.8202 - val_acc: 0.5379\n",
      "Epoch 60/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.7103 - acc: 0.7493\n",
      "Epoch 00060: val_loss did not improve from 1.34566\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.7103 - acc: 0.7493 - val_loss: 4.5443 - val_acc: 0.3898\n",
      "Epoch 61/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.7199 - acc: 0.7476\n",
      "Epoch 00061: val_loss did not improve from 1.34566\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.7199 - acc: 0.7476 - val_loss: 2.6013 - val_acc: 0.4151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.7211 - acc: 0.7467\n",
      "Epoch 00062: val_loss did not improve from 1.34566\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.7211 - acc: 0.7467 - val_loss: 4.0786 - val_acc: 0.3961\n",
      "Epoch 63/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.7156 - acc: 0.7506\n",
      "Epoch 00063: val_loss did not improve from 1.34566\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.7156 - acc: 0.7506 - val_loss: 2.4077 - val_acc: 0.4784\n",
      "Epoch 64/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.7114 - acc: 0.7499\n",
      "Epoch 00064: val_loss did not improve from 1.34566\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.7114 - acc: 0.7499 - val_loss: 2.1656 - val_acc: 0.4717\n",
      "Epoch 65/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.7062 - acc: 0.7528\n",
      "Epoch 00065: val_loss did not improve from 1.34566\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.7062 - acc: 0.7528 - val_loss: 7.1541 - val_acc: 0.2943\n",
      "Epoch 66/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.7420 - acc: 0.7362\n",
      "Epoch 00066: val_loss improved from 1.34566 to 1.12339, saving model to ../model_files/base_cnn_8_seconds_acl_filtered_wesad.hdf5\n",
      "136/136 [==============================] - 2s 13ms/step - loss: 0.7420 - acc: 0.7362 - val_loss: 1.1234 - val_acc: 0.6252\n",
      "Epoch 67/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.7092 - acc: 0.7492\n",
      "Epoch 00067: val_loss did not improve from 1.12339\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.7092 - acc: 0.7492 - val_loss: 2.5491 - val_acc: 0.4271\n",
      "Epoch 68/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.7074 - acc: 0.7482\n",
      "Epoch 00068: val_loss did not improve from 1.12339\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.7074 - acc: 0.7482 - val_loss: 3.5914 - val_acc: 0.4291\n",
      "Epoch 69/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.6955 - acc: 0.7547\n",
      "Epoch 00069: val_loss did not improve from 1.12339\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.6955 - acc: 0.7547 - val_loss: 6.0639 - val_acc: 0.3405\n",
      "Epoch 70/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.7001 - acc: 0.7531\n",
      "Epoch 00070: val_loss did not improve from 1.12339\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.7001 - acc: 0.7531 - val_loss: 5.5263 - val_acc: 0.3532\n",
      "Epoch 71/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.6919 - acc: 0.7558\n",
      "Epoch 00071: val_loss did not improve from 1.12339\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.6919 - acc: 0.7558 - val_loss: 4.6598 - val_acc: 0.3329\n",
      "Epoch 72/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.6973 - acc: 0.7557\n",
      "Epoch 00072: val_loss did not improve from 1.12339\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.6973 - acc: 0.7557 - val_loss: 1.9809 - val_acc: 0.5246\n",
      "Epoch 73/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.6882 - acc: 0.7596\n",
      "Epoch 00073: val_loss did not improve from 1.12339\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.6882 - acc: 0.7596 - val_loss: 4.3974 - val_acc: 0.3875\n",
      "Epoch 74/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.6990 - acc: 0.7543\n",
      "Epoch 00074: val_loss did not improve from 1.12339\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.6990 - acc: 0.7543 - val_loss: 4.1274 - val_acc: 0.3688\n",
      "Epoch 75/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.6900 - acc: 0.7566\n",
      "Epoch 00075: val_loss did not improve from 1.12339\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.6900 - acc: 0.7566 - val_loss: 2.6176 - val_acc: 0.4038\n",
      "Epoch 76/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.6872 - acc: 0.7588\n",
      "Epoch 00076: val_loss did not improve from 1.12339\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.6872 - acc: 0.7588 - val_loss: 3.1988 - val_acc: 0.4364\n",
      "Epoch 77/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.6760 - acc: 0.7630\n",
      "Epoch 00077: val_loss did not improve from 1.12339\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.6760 - acc: 0.7630 - val_loss: 2.7270 - val_acc: 0.4078\n",
      "Epoch 78/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.6835 - acc: 0.7601\n",
      "Epoch 00078: val_loss did not improve from 1.12339\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.6835 - acc: 0.7601 - val_loss: 4.6868 - val_acc: 0.3648\n",
      "Epoch 79/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.6816 - acc: 0.7606\n",
      "Epoch 00079: val_loss did not improve from 1.12339\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.6816 - acc: 0.7606 - val_loss: 3.9473 - val_acc: 0.4224\n",
      "Epoch 80/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.6804 - acc: 0.7597\n",
      "Epoch 00080: val_loss did not improve from 1.12339\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.6804 - acc: 0.7597 - val_loss: 3.0723 - val_acc: 0.4298\n",
      "Epoch 81/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.6663 - acc: 0.7699\n",
      "Epoch 00081: val_loss did not improve from 1.12339\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.6663 - acc: 0.7699 - val_loss: 1.9782 - val_acc: 0.4997\n",
      "Epoch 82/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.6678 - acc: 0.7660\n",
      "Epoch 00082: val_loss did not improve from 1.12339\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.6678 - acc: 0.7660 - val_loss: 2.7410 - val_acc: 0.4171\n",
      "Epoch 83/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.6614 - acc: 0.7694\n",
      "Epoch 00083: val_loss did not improve from 1.12339\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.6614 - acc: 0.7694 - val_loss: 5.2096 - val_acc: 0.3931\n",
      "Epoch 84/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.6641 - acc: 0.7659\n",
      "Epoch 00084: val_loss did not improve from 1.12339\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.6641 - acc: 0.7659 - val_loss: 1.6929 - val_acc: 0.5549\n",
      "Epoch 85/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.6566 - acc: 0.7687\n",
      "Epoch 00085: val_loss did not improve from 1.12339\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.6566 - acc: 0.7687 - val_loss: 1.7672 - val_acc: 0.5533\n",
      "Epoch 86/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.6624 - acc: 0.7703\n",
      "Epoch 00086: val_loss did not improve from 1.12339\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.6624 - acc: 0.7703 - val_loss: 3.3551 - val_acc: 0.3981\n",
      "Epoch 87/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.6687 - acc: 0.7653\n",
      "Epoch 00087: val_loss did not improve from 1.12339\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.6687 - acc: 0.7653 - val_loss: 4.5660 - val_acc: 0.3635\n",
      "Epoch 88/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.6571 - acc: 0.7688\n",
      "Epoch 00088: val_loss did not improve from 1.12339\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.6571 - acc: 0.7688 - val_loss: 4.3540 - val_acc: 0.3635\n",
      "Epoch 89/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.6566 - acc: 0.7695\n",
      "Epoch 00089: val_loss did not improve from 1.12339\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.6566 - acc: 0.7695 - val_loss: 4.2561 - val_acc: 0.3865\n",
      "Epoch 90/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.6491 - acc: 0.7746\n",
      "Epoch 00090: val_loss did not improve from 1.12339\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.6491 - acc: 0.7746 - val_loss: 2.5588 - val_acc: 0.4854\n",
      "Epoch 91/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.6401 - acc: 0.7766\n",
      "Epoch 00091: val_loss did not improve from 1.12339\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.6401 - acc: 0.7766 - val_loss: 3.1341 - val_acc: 0.3951\n",
      "Epoch 92/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.6420 - acc: 0.7760\n",
      "Epoch 00092: val_loss did not improve from 1.12339\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.6420 - acc: 0.7760 - val_loss: 4.5590 - val_acc: 0.4035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.6428 - acc: 0.7739\n",
      "Epoch 00093: val_loss did not improve from 1.12339\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.6428 - acc: 0.7739 - val_loss: 5.9145 - val_acc: 0.3402\n",
      "Epoch 94/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.6475 - acc: 0.7745\n",
      "Epoch 00094: val_loss did not improve from 1.12339\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.6475 - acc: 0.7745 - val_loss: 2.2309 - val_acc: 0.5210\n",
      "Epoch 95/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.6437 - acc: 0.7718\n",
      "Epoch 00095: val_loss did not improve from 1.12339\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.6437 - acc: 0.7718 - val_loss: 4.5570 - val_acc: 0.3589\n",
      "Epoch 96/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.6490 - acc: 0.7743\n",
      "Epoch 00096: val_loss did not improve from 1.12339\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.6490 - acc: 0.7743 - val_loss: 3.8272 - val_acc: 0.4457\n",
      "Epoch 97/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.6468 - acc: 0.7747\n",
      "Epoch 00097: val_loss did not improve from 1.12339\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.6468 - acc: 0.7747 - val_loss: 4.1959 - val_acc: 0.4105\n",
      "Epoch 98/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.6337 - acc: 0.7757\n",
      "Epoch 00098: val_loss did not improve from 1.12339\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.6337 - acc: 0.7757 - val_loss: 3.0822 - val_acc: 0.4537\n",
      "Epoch 99/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.6312 - acc: 0.7798\n",
      "Epoch 00099: val_loss did not improve from 1.12339\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.6312 - acc: 0.7798 - val_loss: 4.2944 - val_acc: 0.3782\n",
      "Epoch 100/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.6185 - acc: 0.7846\n",
      "Epoch 00100: val_loss did not improve from 1.12339\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.6185 - acc: 0.7846 - val_loss: 3.1843 - val_acc: 0.4571\n",
      "Epoch 101/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.6298 - acc: 0.7773\n",
      "Epoch 00101: val_loss did not improve from 1.12339\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.6298 - acc: 0.7773 - val_loss: 4.0437 - val_acc: 0.3995\n",
      "Epoch 102/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.6398 - acc: 0.7731\n",
      "Epoch 00102: val_loss did not improve from 1.12339\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.6398 - acc: 0.7731 - val_loss: 3.9628 - val_acc: 0.4015\n",
      "Epoch 103/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.6209 - acc: 0.7874\n",
      "Epoch 00103: val_loss did not improve from 1.12339\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.6209 - acc: 0.7874 - val_loss: 1.2266 - val_acc: 0.6278\n",
      "Epoch 104/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.6138 - acc: 0.7862\n",
      "Epoch 00104: val_loss did not improve from 1.12339\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.6138 - acc: 0.7862 - val_loss: 4.5482 - val_acc: 0.3998\n",
      "Epoch 105/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.6237 - acc: 0.7814\n",
      "Epoch 00105: val_loss did not improve from 1.12339\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.6237 - acc: 0.7814 - val_loss: 3.7953 - val_acc: 0.4121\n",
      "Epoch 106/400\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.6271 - acc: 0.7789\n",
      "Epoch 00106: val_loss did not improve from 1.12339\n",
      "136/136 [==============================] - 2s 12ms/step - loss: 0.6271 - acc: 0.7789 - val_loss: 6.0068 - val_acc: 0.3642\n",
      "Epoch 00106: early stopping\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=40)\n",
    "callbacks_list = [es,checkpoint]\n",
    "history = model.fit(train_x,train_participant,validation_data=(val_x,val_participant), epochs=400, batch_size=200,\n",
    "          callbacks=callbacks_list,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(filepath)\n",
    "test_participant_pred = model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[139   3   8  52   1   6   9  13  18  92 180   2   1]\n",
      " [  0 350   0  11  63   1   0   1   8   4   9   0 201]\n",
      " [ 11   0 133  63   0   5   5  16   8  27 248   6   1]\n",
      " [  1   1   0 445   0   3   0   0  26  60 167   3   0]\n",
      " [  0  62   0   1 265   0   1   0   9   2   7   0 244]\n",
      " [ 17   1   1  22   2 196   3   2  26 139 114  22   8]\n",
      " [ 20   0   4   7   0   6  90  10   4 136 270   2   0]\n",
      " [ 17   1   8  39   0   8  10 118   8  92 305  19   0]\n",
      " [  2   0   4  74   0   2   0   2 251  98 124   5   0]\n",
      " [  6   1   2  53   0  10   1   4  31 253 128  32   0]\n",
      " [  0   4   0  28  10   2   6   1  11  90 388   5   1]\n",
      " [  0   0   0  33   0  10   0   4  41 114 184 221   0]\n",
      " [  0  69   0   6 106   1   0   0  12   3   3   0 354]]\n",
      "0.42655480090557996\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(np.argmax(test_participant,axis=1),np.argmax(test_participant_pred,axis=1)))\n",
    "print(accuracy_score(np.argmax(test_participant,axis=1),np.argmax(test_participant_pred,axis=1)))\n",
    "#filtered_acl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[410   5  19   4  12   4  17   6  24   6   1   6  10]\n",
      " [  0 575   6   5  46   0   0   1  10   0   3   0   2]\n",
      " [  8  23 436   6   6   1   3   6  13  10   0  10   1]\n",
      " [  3  10   2 592  13   2   7   1  27  17   8  21   3]\n",
      " [  3  13   7   3 515   0  15   2   7   2  14   2   8]\n",
      " [  7   0   1   0   2 490  13   1   1   5   8   5  20]\n",
      " [  7   2   3   6   3  14 434   0  18  21   3   0  38]\n",
      " [  5   0  15   3  10   7   2 548   0   3  12  20   0]\n",
      " [ 16  17  15  26   6   0  18   0 448   4   2   0  10]\n",
      " [  4   0  18   9   0  11   9   2   6 453   2   6   1]\n",
      " [  3   0   0   7  14  19   8   3   2   6 421  25  38]\n",
      " [  8   4  31  17   6   6   1   8   5  20  18 477   6]\n",
      " [  2   0   0   3   4  14  21   0  11   1   8   0 490]]\n",
      "0.8375282993740845\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(np.argmax(test_participant,axis=1),np.argmax(test_participant_pred,axis=1)))\n",
    "print(accuracy_score(np.argmax(test_participant,axis=1),np.argmax(test_participant_pred,axis=1)))\n",
    "#filtered ppg+filtered_acl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[496   0   5   3   0   2   4   1   4   3   0   6   0]\n",
      " [  0 644   0   1   2   0   0   0   0   0   0   0   1]\n",
      " [  1   0 510   2   0   0   0   1   4   0   5   0   0]\n",
      " [  1   0   1 691   0   4   0   0   2   5   0   2   0]\n",
      " [  0  32   0   0 556   0   0   0   0   0   0   0   3]\n",
      " [  1   0   0   3   0 535   3   0   1   1   7   2   0]\n",
      " [  7   0   2   0   0   7 526   1   3   3   0   0   0]\n",
      " [ 13   0   5   2   0   1   0 592   1   0   5   6   0]\n",
      " [  8   0   6   5   0   1   1   1 536   0   3   1   0]\n",
      " [  0   0   5   3   0   0   2   0   0 504   5   2   0]\n",
      " [  1   0   0   4   0   4   2   4   0   3 523   5   0]\n",
      " [  1   0   1   9   0   4   2   6   3   1   1 579   0]\n",
      " [  0   0   0   0   5   0   0   0   0   0   0   0 549]]\n",
      "0.9643094952723399\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(np.argmax(test_participant,axis=1),np.argmax(test_participant_pred,axis=1)))\n",
    "print(accuracy_score(np.argmax(test_participant,axis=1),np.argmax(test_participant_pred,axis=1)))\n",
    "#filtered ppg+acl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[419   5  19   2  15   1  11  11  34   0   2   3   2]\n",
      " [  6 584   9   5  32   0   4   2   4   0   1   0   1]\n",
      " [ 12  24 428   4   6   2   1  15   6   9   2  11   3]\n",
      " [ 12   6   3 593  16   4   6   1  29  12   7   9   8]\n",
      " [  6  37   1   7 506   0   9   4   7   0   5   3   6]\n",
      " [ 11   0   2   1   2 486  17   4   1   6   8   3  12]\n",
      " [ 15   1   0   7   7  25 437   1  19  11   1   1  24]\n",
      " [  7   3   8   9  16   4   1 544   0   2  15  15   1]\n",
      " [ 25  18  16  24   3   0  14   1 446   2   0   1  12]\n",
      " [ 18   1   8   9   0  22   6   3   6 432   0  12   4]\n",
      " [  6   2   1  13  12  10   4   8   3   1 444  25  17]\n",
      " [ 16   4  17  18   5   4   3  23   3   8  24 478   4]\n",
      " [ 11   1   1   3   5  17  12   0  17   1  11   0 475]]\n",
      "0.8352643494473299\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(np.argmax(test_participant,axis=1),np.argmax(test_participant_pred,axis=1)))\n",
    "print(accuracy_score(np.argmax(test_participant,axis=1),np.argmax(test_participant_pred,axis=1)))\n",
    "#filtered ppg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[504   0   0   1   0   1   2   5   0   1   1   9   0]\n",
      " [  0 644   0   0   2   0   0   0   0   0   0   1   1]\n",
      " [  8   0 488   4   0   1   0   6   8   4   1   3   0]\n",
      " [  1   0   0 689   0   4   0   0   3   3   1   5   0]\n",
      " [  0   7   0   0 581   0   0   0   0   0   0   0   3]\n",
      " [  0   0   0   0   0 545   1   4   2   0   1   0   0]\n",
      " [  2   0   0   0   0  10 531   0   2   4   0   0   0]\n",
      " [  5   0   0   0   0   0   3 608   1   3   0   5   0]\n",
      " [  3   3   2   3   0   2   4   1 544   0   0   0   0]\n",
      " [  1   0   1   2   0   1   1   1   0 510   1   3   0]\n",
      " [  0   0   0   3   0   6   0   0   0   1 534   2   0]\n",
      " [  1   0   0   4   0   1   0   7   0   1   1 592   0]\n",
      " [  0   0   0   0   3   0   0   0   0   0   0   0 551]]\n",
      "0.9749633772805966\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(np.argmax(test_participant,axis=1),np.argmax(test_participant_pred,axis=1)))\n",
    "print(accuracy_score(np.argmax(test_participant,axis=1),np.argmax(test_participant_pred,axis=1)))\n",
    "#ppg+acl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[473   1  12   0   8   3   3   2  11   2   2   7   0]\n",
      " [  0 631   2   1   4   0   0   1   5   0   3   0   1]\n",
      " [  2   7 491   2   7   0   0   1   7   2   2   2   0]\n",
      " [  1   1   1 678   0   3   0   0  14   1   6   0   1]\n",
      " [  1  10   0   0 572   0   0   0   3   0   1   4   0]\n",
      " [  0   0   0   5   0 535   5   1   0   1   1   0   5]\n",
      " [  7   0   1   1   0   5 515   0  14   2   0   0   4]\n",
      " [  4   0   6   0   7   2   2 579   1   1   3  20   0]\n",
      " [  2   1   3   3   1   1   3   0 547   0   0   1   0]\n",
      " [  4   1   6  12   0   0   5   0   1 486   0   5   1]\n",
      " [  2   3   0   4   2   4   1   2   3   1 508   4  12]\n",
      " [  2   1   6   2   6   0   0   1   3   1   2 583   0]\n",
      " [  0   0   0   3   0   6   0   0   2   0   2   0 541]]\n",
      "0.9507257957118125\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(np.argmax(test_participant,axis=1),np.argmax(test_participant_pred,axis=1)))\n",
    "print(accuracy_score(np.argmax(test_participant,axis=1),np.argmax(test_participant_pred,axis=1)))\n",
    "#ppg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[414   0  14  18   0  26   7   7   6  17   7   8   0]\n",
      " [  0 592   0   0  32   0   0   0   0   0   0   0  24]\n",
      " [  5   0 454  12   0  11  10   5  12   7   4   3   0]\n",
      " [  4   1   4 641   3   5   7   5   1  12   6  17   0]\n",
      " [  0  29   0   0 519   0   0   0   0   0   0   0  43]\n",
      " [ 15   0  17  13   0 444  15  10  10   8   8  13   0]\n",
      " [ 32   0  10   4   0  16 449   9  15   8   5   1   0]\n",
      " [ 22   0   3  14   0  10  13 489   8  41   8  17   0]\n",
      " [  7   0  16   5   0  11  20   2 445  26  14  16   0]\n",
      " [  9   0  20   5   0   6   9  15  28 419   4   6   0]\n",
      " [  4   3   9  10   0   8   3  18   8   2 472   9   0]\n",
      " [  7   0   0  20   0   8  12   6  17   8   6 523   0]\n",
      " [  0  35   0   1  16   0   0   0   0   0   0   0 502]]\n",
      "0.8473831402317219\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(np.argmax(test_participant,axis=1),np.argmax(test_participant_pred,axis=1)))\n",
    "print(accuracy_score(np.argmax(test_participant,axis=1),np.argmax(test_participant_pred,axis=1)))\n",
    "#acl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
